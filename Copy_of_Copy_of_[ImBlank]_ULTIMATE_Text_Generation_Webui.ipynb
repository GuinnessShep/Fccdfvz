{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLM text generation notebook for Google Colab\n",
        "\n",
        "This notebook uses [https://github.com/oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) to run the [Pygmalion](https://huggingface.co/models?search=pygmalion) conversational models (NSFW) in chat mode.\n",
        "\n",
        "Run all the cells and a public gradio URL will appear at the bottom in around 5 minutes.\n",
        "\n",
        "https://status.gradio.app/\n",
        "\n",
        "## Parameters\n",
        "\n",
        "* **save_logs_to_google_drive**: saves your chat logs, characters, and softprompts to Google Drive automatically, so that they will persist across sessions.\n",
        "* **text_streaming**: streams the text output in real time instead of waiting for the full response to be completed.\n",
        "](https://oobabooga.github.io/silero-samples/).\n",
        "* **activate_sending_pictures**: adds a menu for sending pictures to the bot, which are automatically captioned using BLIP.\n",
        "* **activate_character_bias**: an extension that adds an user-defined, hidden string at the beginning of the bot's reply with the goal of biasing the rest of the response.\n",
        "* **chat_language**: if different than English, activates automatic translation using Google Translate, allowing you to communicate with the bot in a different language.\n",
        "\n",
        "## Characters\n",
        "\n",
        "You can use the following websites to create characters compatible with this web UI:\n",
        "\n",
        "* [JSON character creator](https://oobabooga.github.io/character-creator.html)\n",
        "* [AI Character Editor](https://zoltanai.github.io/character-editor/)\n",
        "\n",
        "## Credits\n",
        "\n",
        "Based on the [original notebook by 81300](https://colab.research.google.com/github/81300/AI-Notebooks/blob/main/Colab-TextGen-GPU.ipynb).\n",
        "# -------------------------------------\n",
        "***Reminder:*** *if the notebook colab is not working in out of no where for some reason, then i'm probably doing something like adding stuff, removing stuff etc..or it's just colab being weird again*"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###**‚ö†Ô∏èDO NOT USE FOR NOW BECAUSE I AM CURRENTLY MERGING SILLYTAVERN‚ö†Ô∏è [05/20/2023]**\n",
        "+ Added the **NEW** Pygmalion Models, these are the **Pygmalion-13B** and **Metharme-13B**\n",
        "\n",
        "+ Replaced Gpt 4 x alpaca with Wizard-Vicuna-13B-Uncensored-GPTQ\n",
        "\n",
        "+ ~~Reverted Ooba back to the latest version~~ Nevermind, everything is not working for some reason \n",
        "\n",
        "+ Added PygmalionCoT-7B\n",
        "\n",
        "\n",
        "#####**Things needed to be done:**\n",
        "+ Add AutoGPT (Destroy Humanity)\n",
        "+ Fix EdgeGPT extension (Bots can use Bing AI Search & GoogleWordCloud)\n",
        "+ Add superbooga (Infinite pseudo context)\n",
        "+ Add AnnoyLTM (Fking finally Bots are not a dementia patient anymore wooo!!)\n",
        "+ Add KoboldAI\n",
        "\n",
        "\n",
        "#**Some Information About The Models**\n",
        "**WizardVicuna 13B**\n",
        "+ A **NEW** [WizardLM](https://github.com/nlpxucan/WizardLM) model that was trained with extra steps; Wizard's dataset + ChatGPT's conversation extension + Vicuna's tuning method, though its not entirely a merged model between WizardLM and VicunaLM but rather, the model used the same fine tuning method as Vicuna.\n",
        "\n",
        "+ The model's benchmark is approximately (Ôø¢_Ôø¢) 3% behind GPT 3.5 and 7% better than Vicuna 13B model and 9% better than WizardLM 7B, for more detailed information. Click this [Github Page](https://github.com/melodysdreamj/WizardVicunaLM)\n",
        "\n",
        "+ A bit disappointing to be honest....i assume that the 4bit quantization reduce it's quality response in exchange for faster response time, I would rather use PygmalionCot 7B instead.\n",
        "\n",
        "**Pygmalion 13B**\n",
        "+ Pygmalion 13B is a dialogue model based on Meta's LLaMA-13B, meaning it was trained using Llama-13B as the base model and has been fine-tuned using a subset of the data from Pygmalion-6B-v8-pt4\n",
        "+ It is Currently **VERSION 1**\n",
        "+ Reminder: the quality response might differ because this colab notebook used the 4bit version.\n",
        "\n",
        "**Metharme 13B**\n",
        "+ Upgraded version of Metharme 7B model\n",
        "\n",
        "**PygmalionCoT 7B**\n",
        "+ A merged model between Pygmalion 7B and [And kaiokendev's 7b SuperCOT-LoRA (Chain of thought)](https://huggingface.co/kaiokendev/SuperCOT-LoRA)\n",
        "+ The model is much more coherent than Pygmalion 7B because of the LoRa that was merged, the LoRa was trained for Llama based models to improve instructions comprehension of bots.\n",
        "+Tried it and it's pretty good, however it's NSFW factor is a bit toned down but overall the bots are more in character which is an improvement.\n",
        "\n",
        "**Pygmalion 7B**\n",
        "+ Pygmalion model that has the same architecture as a Llama model\n",
        "+ Better Convo, verbose\n",
        "+ Better Roleplaying \n",
        "+ However a bit difficult in initiating NSFW RP and might resorted to roasting you if you forced the bot to do it.\n",
        "\n",
        "**Metharme 7B**\n",
        "+ A New [EXPERIMENTAL] model that was trained by the pygmalion devs for storywriting, RPG text simulator, roleplay, etc..\n",
        "+ Seems ok\n",
        "\n",
        "**Pygmalion 350M (CPU)**\n",
        "+ Don't even bother using this, it's mainly for debug stuff because of its small size, it is a very convenient model for doing maintenance stuff.\n",
        "\n",
        "**Pygmalion 6B main (Outdated)**\n",
        "+ The OG Pygmalion Model that specialized in NSFW roleplay\n",
        "\n",
        "**Pygmalion 6B dev (Outdated)**\n",
        "+ Not really that much different when compared to either the main or the original branch.\n",
        "\n",
        "**Pygmalion 6B original (Outdated)**\n",
        "+ The OG OG Pygmalion Model that specialized in NSFW roleplay and it's a bit too overtrained, that resulted in outputting a lengthy response"
      ],
      "metadata": {
        "id": "inqQgQXsl2qV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "# Animations ‚Üì‚Üì\n",
        "# &nbsp; &nbsp;<img src=\"https://huggingface.co/Imablank/AnythingV3-1/resolve/main/ooba/game-start.gif?size=90\" width=\"90\"/> \n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "6d4c8985-80f8-4c75-94fe-46ed3359b151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##**üöÄStart TextGenWebUIü™∂**\n",
        "from IPython.display import clear_output, display, HTML\n",
        "from IPython.utils import capture\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import os, sys, base64, subprocess, json, shutil, requests, time, pathlib\n",
        "\n",
        "#PARAMS\n",
        "#@markdown ####**üìñSetup Your UserProfile**\n",
        "\n",
        "UserName = \"\" #@param{type:'string'}\n",
        "Upload_ProfilePic = False #@param{type:'boolean'}\n",
        "save_logs_to_google_drive = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ######<font color=gray>*Save your data such as your name, logs, characters, etc.. in google drive*\n",
        "#@markdown ---\n",
        "#@markdown ####**‚öôÔ∏èConfigure and RunüöÄ**\n",
        "model = \"PygmalionCoT-7b\" #@param [\"Pygmalion-13B-4bit\", \"Metharme-13B-4bit\", \"PygmalionCoT-7b\", \"Pygmalion-7B\", \"Metharme-7B\", \"Wizard-Vicuna-13B-Uncensored-GPTQ\", \"Pygmalion-350m(CPU)\", \"Pygmalion_6B_main_Sharded\", \"Pygmalion_6B_original_Sharded\", \"Pygmalion_6B_dev_Sharded\"]\n",
        "text_streaming = False #@param {type:\"boolean\"}\n",
        "activate_sending_pictures = False #@param {type:\"boolean\"}\n",
        "activate_character_bias = False #@param {type:\"boolean\"}\n",
        "\n",
        "GrantBot_Access_to_Bing = False #param {type:\"boolean\"}\n",
        "\n",
        "chat_language = \"English\" #@param ['Afrikaans', 'Albanian', 'Amharic', 'Arabic', 'Armenian', 'Azerbaijani', 'Basque', 'Belarusian', 'Bengali', 'Bosnian', 'Bulgarian', 'Catalan', 'Cebuano', 'Chinese (Simplified)', 'Chinese (Traditional)', 'Corsican', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English', 'Esperanto', 'Estonian', 'Finnish', 'French', 'Frisian', 'Galician', 'Georgian', 'German', 'Greek', 'Gujarati', 'Haitian Creole', 'Hausa', 'Hawaiian', 'Hebrew', 'Hindi', 'Hmong', 'Hungarian', 'Icelandic', 'Igbo', 'Indonesian', 'Irish', 'Italian', 'Japanese', 'Javanese', 'Kannada', 'Kazakh', 'Khmer', 'Korean', 'Kurdish', 'Kyrgyz', 'Lao', 'Latin', 'Latvian', 'Lithuanian', 'Luxembourgish', 'Macedonian', 'Malagasy', 'Malay', 'Malayalam', 'Maltese', 'Maori', 'Marathi', 'Mongolian', 'Myanmar (Burmese)', 'Nepali', 'Norwegian', 'Nyanja (Chichewa)', 'Pashto', 'Persian', 'Polish', 'Portuguese (Portugal, Brazil)', 'Punjabi', 'Romanian', 'Russian', 'Samoan', 'Scots Gaelic', 'Serbian', 'Sesotho', 'Shona', 'Sindhi', 'Sinhala (Sinhalese)', 'Slovak', 'Slovenian', 'Somali', 'Spanish', 'Sundanese', 'Swahili', 'Swedish', 'Tagalog (Filipino)', 'Tajik', 'Tamil', 'Telugu', 'Thai', 'Turkish', 'Ukrainian', 'Urdu', 'Uzbek', 'Vietnamese', 'Welsh', 'Xhosa', 'Yiddish', 'Yoruba', 'Zulu']\n",
        "activate_google_translate = (chat_language != \"English\")\n",
        "RunWebUI = True #@param {type:\"boolean\"}\n",
        "GetAPI = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=#eab676>Run SillyTavern instead of gradio or Get the oobabooga public API .\n",
        "Debug = False #@param{type:'boolean'}\n",
        "#@markdown > <font color=gray> *no bocchi? :(*\n",
        "\n",
        "#@markdown &nbsp; &nbsp;<img src=\"https://huggingface.co/Imablank/AnythingV3-1/resolve/main/ooba/bocchi-the-rock-crying.gif?size=90\" width=\"90\"/> \n",
        "\n",
        "ot=\" \"\n",
        "load_in_8bit = True\n",
        "if \"Wizard-Vicuna-13B-Uncensored-GPTQ\" in model or \"4bit\" in model: load_in_8bit = False\n",
        "old_ooba = True\n",
        "kobold_memory = True\n",
        "\n",
        "#Main Directories:\n",
        "base_folder = \"/content/drive/MyDrive/oobabooga-data\"\n",
        "main_dir = \"/content/text-generation-webui/\"\n",
        "cache = f\"{main_dir}cache\"\n",
        "\n",
        "#PART 1 INSTALL TextGeN\n",
        "if save_logs_to_google_drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "def install_silly():\n",
        "  ForceInitSteps = []\n",
        "  class IncrementialInstall:\n",
        "    def __init__(self, root = \"/\", tasks = [], force = []):\n",
        "        self.tasks = tasks\n",
        "        self.path = os.path.join(root, \".ii\")\n",
        "        self.completed = list(filter(lambda x: not x in force, self.__completed()))\n",
        "\n",
        "    def __completed(self):\n",
        "        try:\n",
        "            with open(self.path) as f:\n",
        "                return json.load(f)\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "    def addTask(self, name, func):\n",
        "        self.tasks.append({\"name\": name, \"func\": func})\n",
        "\n",
        "    def run(self):\n",
        "        todo = list(filter(lambda x: not x[\"name\"] in self.completed, self.tasks))\n",
        "        try:\n",
        "            for task in todo:\n",
        "                task[\"func\"]()\n",
        "                self.completed.append(task[\"name\"])\n",
        "        finally:\n",
        "            with open(self.path, \"w\") as f:\n",
        "                json.dump(self.completed, f)\n",
        "\n",
        "  def create_paths(paths):\n",
        "    for directory in paths:\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "\n",
        "  def link(srcDir, destDir, files):\n",
        "    '''\n",
        "        Link source to dest copying dest to source if not present first\n",
        "    '''\n",
        "    for file in files:\n",
        "        source = os.path.join(srcDir, file)\n",
        "        dest = os.path.join(destDir, file)\n",
        "        if not os.path.exists(source):\n",
        "            !cp -r \"$dest\" \"$source\"\n",
        "        !rm -rf \"$dest\"\n",
        "        !ln -fs \"$source\" \"$dest\"\n",
        "  from google.colab import drive\n",
        "  if not save_logs_to_google_drive:\n",
        "    create_paths([\n",
        "            \"/content/SillyTavern-Data\"\n",
        "    ])\n",
        "  ii = IncrementialInstall(force=ForceInitSteps)\n",
        "# ---\n",
        "# SillyTavern py modules\n",
        "  def cloneTavern():\n",
        "    %cd /\n",
        "    !git clone https://github.com/Cohee1207/SillyTavern\n",
        "    !wget https://huggingface.co/Imablank/AnythingV3-1/resolve/main/ooba/globals.py && wget https://huggingface.co/Imablank/AnythingV3-1/resolve/main/ooba/extras_server.py \n",
        "  ii.addTask(\"Clone SillyTavern\", cloneTavern)\n",
        "  ii.run()\n",
        "  kargs = [\"/content/ckds\"]\n",
        "  kargs += [\"--localtunnel\", \"yes\"]\n",
        "\n",
        "# ---\n",
        "# nodejs\n",
        "  %cd /\n",
        "  def installNode():\n",
        "    !npm install -g n\n",
        "    !n 19\n",
        "    !node --version\n",
        "  installNode()\n",
        "# ---\n",
        "# TavernAI extras\n",
        "  import globals\n",
        "  globals.params = []\n",
        "  globals.params.append('--cpu')\n",
        "  %cd /SillyTavern\n",
        "  if save_logs_to_google_drive:\n",
        "    %env googledrive=2\n",
        "    def setupTavernPaths():\n",
        "        %cd /SillyTavern\n",
        "        tdrive = \"/content/drive/MyDrive/SillyTavern\"\n",
        "        create_paths([\n",
        "                tdrive,\n",
        "                os.path.join(\"public\", \"groups\"),\n",
        "                os.path.join(\"public\", \"group chats\")\n",
        "        ])\n",
        "        link(tdrive, \"public\", [\n",
        "                \"settings.json\",\n",
        "                \"backgrounds\",\n",
        "                \"characters\",\n",
        "                \"chats\",\n",
        "                \"User Avatars\",\n",
        "                \"worlds\",\n",
        "                \"group chats\",\n",
        "                \"groups\",\n",
        "        ])\n",
        "    ii.addTask(\"Setup Tavern Paths\", setupTavernPaths)\n",
        "  def installTavernDependencies():\n",
        "    %cd /SillyTavern\n",
        "    !npm install\n",
        "    !npm install -g localtunnel\n",
        "  ii.addTask(\"Install Tavern Dependencies\", installTavernDependencies)\n",
        "  ii.run()\n",
        "\n",
        "def universal_download(link1, path1, nan3):\n",
        "  !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {link1} -d {path1} -o {nan3}\n",
        "\n",
        "def edgegpt():\n",
        "  import zipfile\n",
        "  %cd /content/\n",
        "  !wget https://huggingface.co/Imablank/AnythingV3-1/resolve/main/ooba/EdgeGPT_working.zip\n",
        "  with zipfile.ZipFile('/content/EdgeGPT_working.zip', 'r') as zip:\n",
        "    zip.extractall(\"/content/text-generation-webui/extensions\")\n",
        "  !pip install -r {main_dir}extensions/EdgeGPT/requirements.txt && rm -rf /content/EdgeGPT_working.zip\n",
        "  %cd {main_dir}\n",
        "\n",
        "def install_ooba():\n",
        "  !apt-get -y install -qq aria2\n",
        "     \n",
        "  !mkdir {main_dir}cache\n",
        "  if save_logs_to_google_drive:\n",
        "    if not os.path.exists(f\"{base_folder}\"):\n",
        "      os.mkdir(f\"{base_folder}\")\n",
        "    if not os.path.exists(f\"{base_folder}/logs\"):\n",
        "      os.mkdir(f\"{base_folder}/logs\")\n",
        "    if not os.path.exists(f\"{base_folder}/User\"):\n",
        "      os.mkdir(f\"{base_folder}/User\")\n",
        "    if not os.path.exists(f\"{base_folder}/softprompts\"):\n",
        "      os.mkdir(f\"{base_folder}/softprompts\")\n",
        "    if not os.path.exists(f\"{base_folder}/characters\"):\n",
        "      shutil.move(\"text-generation-webui/characters\", f\"{base_folder}/characters\")\n",
        "    else:\n",
        "      !rm -r \"text-generation-webui/characters\"\n",
        "    \n",
        "    !rm -r \"text-generation-webui/softprompts\"\n",
        "    !ln -s \"$base_folder/logs\" \"text-generation-webui/\"\n",
        "    !ln -s \"$base_folder/softprompts\" \"text-generation-webui/softprompts\"\n",
        "    !ln -s \"$base_folder/characters\" \"text-generation-webui/characters\"\n",
        "    !ln -s \"$base_folder/User\" .\n",
        "  else:\n",
        "    !mkdir text-generation-webui/logs\n",
        "    \n",
        "  !ln -s text-generation-webui/logs .\n",
        "  !ln -s text-generation-webui/characters .\n",
        "  !ln -s text-generation-webui/models .\n",
        "    \n",
        "  %rm -r sample_data\n",
        "  %cd {main_dir}\n",
        "  !wget https://oobabooga.github.io/settings-colab.json -O settings-colab-template.json\n",
        "\n",
        "  !rm -rf \"/content/text-generation-webui/presets/Default.txt\" \"/content/text-generation-webui/settings-colab-template.json\" \"/content/text-generation-webui/settings-template.json\"\n",
        "\n",
        "  link=\"https://huggingface.co/Imablank/AnythingV3-1/raw/main/GPT4.txt, https://huggingface.co/Imablank/AnythingV3-1/raw/main/Alpha.json, https://huggingface.co/Imablank/AnythingV3-1/resolve/main/Alpha.png, https://huggingface.co/Imablank/AnythingV3-1/raw/main/settings.json\"\n",
        "  path=\"/content/text-generation-webui/presets/, /content/characters/,  /content/characters/, /content/text-generation-webui/\"\n",
        "  nan3=\"GPT4.txt, Alpha.json, Alpha.png, settings.json\"; link=link.split(', '); path=path.split(', '); nan3=nan3.split(', ')\n",
        "  for link, path, nan3 in zip(link, path, nan3):\n",
        "    universal_download(link, path, nan3)\n",
        "\n",
        "  %cd /content/text-generation-webui\n",
        "  !pip install -r requirements.txt\n",
        "  !pip install -r extensions/google_translate/requirements.txt\n",
        "  !pip install -r extensions/api/requirements.txt\n",
        "  \n",
        "  if \"Wizard-Vicuna-13B-Uncensored-GPTQ\" == model or \"4bit\" in model:\n",
        "    !mkdir /content/text-generation-webui/repositories\n",
        "    %cd repositories\n",
        "    !git clone -b v1.0 https://github.com/camenduru/GPTQ-for-LLaMa.git\n",
        "    %cd GPTQ-for-LLaMa\n",
        "    !python setup_cuda.py install\n",
        "\n",
        "if not os.path.exists(main_dir) or \"GPT4-x-alpaca/ROLEPLAY\" == model and not os.path.exists(\"/content/text-generation-webui/repositories\"):    \n",
        "  !cd /content \\\n",
        "    && git clone https://github.com/oobabooga/text-generation-webui \\\n",
        "    && git clone https://github.com/theubie/complex_memory {main_dir}/extensions/complex_memory     \n",
        "  \n",
        "  if old_ooba:\n",
        "    %cd {main_dir}\n",
        "    !git checkout f052ab9c8fed3dedc446c3847f10ab22e42bfb37\n",
        "\n",
        "  %cd /content\n",
        "  clear_output()\n",
        "\n",
        "  if not Debug:\n",
        "    display(HTML(\"<img src='https://huggingface.co/Imablank/AnythingV3-1/resolve/main/BoochiLoading.gif' width='280px'/>\"))\n",
        "    print(\"\\033[92m[Installing oobabooga]\")\n",
        "    with capture.capture_output() as cap:\n",
        "      install_ooba(); edgegpt()\n",
        "      if GetAPI: install_silly()\n",
        "  else:\n",
        "    install_ooba(); edgegpt()\n",
        "    if GetAPI: install_silly()\n",
        "  \n",
        "else: clear_output(); print(\"\\033[92m\\n[ ALREADY INSTALLED -- SKIPPING INSTALL.. ]\")\n",
        "\n",
        "!rm -rf \"/content/text-generation-webui/models/config.yaml\"\n",
        "\n",
        "#The Big boy's club\n",
        "def silly_tavern():\n",
        "  p = subprocess.Popen([\"lt\", \"--port\", \"5001\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "  Link=(p.stdout.readline().decode().strip())\n",
        "  # Get external IP address\n",
        "  external_ip = requests.get('https://api.ipify.org').text\n",
        "  external_ip=f\"\\n###Copy Google Colab's Endpoint IP###\\nEndpoint IP: \\033[4;92m{external_ip}\\033[0m\"\n",
        "  print(external_ip, f\"\\n###SillyTavern LINK###\\n{Link}\", sep=\"\\n\")\n",
        "  !node server.js\n",
        "\n",
        "def run_ooba(): \n",
        "  cmd = f\"python server.py --verbose --model {b} --settings {main_dir}cache/{_username_} {' '.join(params)}\"\n",
        "  print(cmd)\n",
        "  !$cmd\n",
        "  if os.path.exists(base_folder):\n",
        "    username_delete = os.listdir(\"/content/drive/MyDrive/oobabooga-data/User\")\n",
        "    try: username_delete.remove(\"pfp_me.png\")\n",
        "    except: pass\n",
        "    for username_delete in username_delete:\n",
        "      !rm -rf {base_folder}/User/$username_delete\n",
        "    if os.path.exists(\"/content/text-generation-webui/cache/pfp_me.png\"):\n",
        "      ot=\" /content/text-generation-webui/cache/pfp_me.png \"\n",
        "    !cp -r /content/text-generation-webui/cache/{_username_}{ot}/content/drive/MyDrive/oobabooga-data/User    \n",
        "\n",
        "\n",
        "def overwrite_profile():\n",
        "  global UserName, _username_\n",
        "  if not username_found == None and not \"\" == UserName:\n",
        "    del_userName= '[{'+username_found+\"}]-settings-colab.json\"\n",
        "    !rm -rf {cache}/{del_userName}   \n",
        "  elif not username_found == None and \"\" == UserName: _username_exists = \"[{\"+username_found+\"}]-settings-colab.json\"; UserName = username_found\n",
        "  elif username_found == None and \"\" == UserName: UserName=\"Anon-san\"\n",
        "  %cd {main_dir}\n",
        "  j = json.loads(open('settings.json', 'r').read())\n",
        "  j[\"google_translate-language string\"] = language_codes[chat_language]\n",
        "  j[\"name1\"] = UserName;\n",
        "  if \" \" in UserName: UserName=UserName.replace(\" \", \"_\")\n",
        "  _username_= '[{'+UserName+\"}]-settings-colab.json\"\n",
        "  with open(_username_, 'w') as f:\n",
        "    f.write(json.dumps(j, indent=4))\n",
        "  !rm -rf {main_dir}cache/{_username_exists}\n",
        "  !mv /content/text-generation-webui/{_username_} /content/text-generation-webui/cache\n",
        "  clear_output()\n",
        "\n",
        "def find_name():\n",
        "  if os.path.exists(base_folder+\"/User\"):\n",
        "    try:\n",
        "      user_profile = os.listdir(base_folder+\"/User\")\n",
        "      for count1 in user_profile:\n",
        "        !cp -r /content/drive/MyDrive/oobabooga-data/User/$count1 /content/text-generation-webui/cache\n",
        "    except: pass\n",
        "  try:\n",
        "    username_find_dir=os.listdir(\"/content/text-generation-webui/cache/\");\n",
        "    username_located=\", \".join(username_find_dir);\n",
        "    username_x1=username_located.index(\"[{\")+2;\n",
        "    username_x2=username_located.index(\"}]\");\n",
        "    username_found=username_located[username_x1:username_x2]\n",
        "  except: username_found = None\n",
        "  finally: return username_found\n",
        "\n",
        "def download_model(type):\n",
        "  tmp_repo=f\"/content/.{huggingface_repo}\"\n",
        "  !git lfs install --skip-smudge && export GIT_LFS_SKIP_SMUDGE=1 && git clone https://huggingface.co/{huggingface_org}/{huggingface_repo} /content/.{huggingface_repo} --branch {huggingface_branch}\n",
        "  !rm -rf {tmp_repo}/PygmalionCoT-7b-ggml-model-f16.bin {tmp_repo}/PygmalionCoT-7b-ggml-q4_2.bin {tmp_repo}/PygmalionCoT-7b-ggml-q5_1.bin {tmp_repo}/PygmalionCoT-7b-ggml-q8_0.bin {tmp_repo}/PygmalionCoT-7b-4bit-128g.safetensors {tmp_repo}/pytorch_model.bin.index.json {tmp_repo}/training_args.bin {tmp_repo}/trainer_state.json {tmp_repo}/all_results.json {tmp_repo}/eval_results.json {tmp_repo}/.gitattributes {tmp_repo}/train_results.json\n",
        "  repo = os.listdir(tmp_repo); clear_output()\n",
        "  for download in repo:    \n",
        "    link_path = f\"https://huggingface.co/{huggingface_org}/{huggingface_repo}/\"\n",
        "    if \".json\" in download or \".txt\" in download:\n",
        "      link_path += f\"raw/{huggingface_branch}/{download}\"\n",
        "    else: link_path += f\"resolve/{huggingface_branch}/{download}\"\n",
        "    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {link_path} -d /content/text-generation-webui/models/{type} -o {download}\n",
        "\n",
        "def upload_profile():  \n",
        "  #Path reader & png format converter\n",
        "  pfp = os.listdir(\"/.pfp/\");\n",
        "  for o in pfp: pfp=o.replace(\" \", \"_\"); pfp=\"/.pfp/\"+o\n",
        "  if os.path.exists(base_folder):\n",
        "    if \".png\" in o:\n",
        "      a = os.rename(pfp, \"pfp_me.png\")\n",
        "      !mv /.pfp/pfp_me.png /content/drive/MyDrive/oobabooga-data/User\n",
        "    else:\n",
        "      Ima1 = Image.open(r\"\"+pfp)\n",
        "      Ima1.save(r\"/content/drive/MyDrive/oobabooga-data/User/pfp_me.png\") #cache\n",
        "  else:\n",
        "    if \".png\" in o:\n",
        "      a = os.rename(pfp, \"pfp_me.png\")\n",
        "      !mv /.pfp/pfp_me.png /content/text-generation-webui/cache\n",
        "    else:\n",
        "      Ima1 = Image.open(r\"\"+pfp)\n",
        "      Ima1.save(r\"/content/text-generation-webui/cache/pfp_me.png\")\n",
        "\n",
        "#PART 2 DOWNLOAD AND START\n",
        "clear_output()\n",
        "!rm -rf /.pfp/ \\\n",
        "  && mkdir /.pfp/\n",
        "username_found = find_name()\n",
        "try:\n",
        "  if Upload_ProfilePic:\n",
        "    %cd /.pfp/\n",
        "    print(\"\\033[92m[Upload Your Profile Picture]\\n\");files.upload();\n",
        "    upload_profile()\n",
        "except: pass\n",
        "\n",
        "# DOWNLOAD\n",
        "%cd {main_dir}\n",
        "if \"Pygmalion-350m(CPU)\" in model:\n",
        "  huggingface_org=\"alpindale\";huggingface_repo=\"pygm-350m-experimental\";huggingface_branch=\"main\"\n",
        "elif \"PygmalionCoT-7b\" in model:\n",
        "  huggingface_org=\"notstoic\";huggingface_repo=\"PygmalionCoT-7b\";huggingface_branch=\"main\"\n",
        "elif \"Wizard-Vicuna-13B-Uncensored-GPTQ\" in model:\n",
        "  huggingface_org=\"TheBloke\";huggingface_repo=\"Wizard-Vicuna-13B-Uncensored-GPTQ\";huggingface_branch= \"main\"\n",
        "elif \"Pygmalion_6B_main_Sharded\" in model:\n",
        "  huggingface_org=\"waifu-workshop\";huggingface_repo=\"pygmalion-6b\";huggingface_branch=\"sharded\"\n",
        "elif \"Pygmalion_6B_original_Sharded\" in model:\n",
        "  huggingface_org=\"waifu-workshop\";huggingface_repo=\"pygmalion-6b\";huggingface_branch=\"original-sharded\"\n",
        "elif \"Pygmalion_6B_dev_Sharded\" in model:\n",
        "  huggingface_org=\"waifu-workshop\";huggingface_repo=\"pygmalion-6b\";huggingface_branch=\"dev-sharded\"\n",
        "elif \"Pygmalion-7B\" in model:\n",
        "  huggingface_org=\"TehVenom\";huggingface_repo=\"Pygmalion-7b-Merged-Safetensors\";huggingface_branch=\"main\"\n",
        "elif \"Metharme-7B\" in model:\n",
        "  huggingface_org=\"Imablank\";huggingface_repo=\"Metharme-7B-MERGED_WEIGHTS\";huggingface_branch=\"main\"\n",
        "elif \"Pygmalion-13B-4bit\" in model:\n",
        "  huggingface_org=\"notstoic\";huggingface_repo=\"pygmalion-13b-4bit-128g\";huggingface_branch=\"main\"\n",
        "elif \"Metharme-13B-4bit\" in model:\n",
        "  huggingface_org=\"TehVenom\";huggingface_repo=\"Metharme-13b-Merged\";huggingface_branch=\"main\"\n",
        "\n",
        "\n",
        "if not \"main\" == huggingface_branch: b = huggingface_repo+'_'+huggingface_branch\n",
        "else: b = huggingface_repo\n",
        "\n",
        "if not os.path.exists(\"/content/models/\"+huggingface_repo): download_model(b); clear_output(); print(\"\\033[96m\\n[\"+model+\" Installed]\\n\")\n",
        "\n",
        "clear_output()\n",
        "\n",
        "language_codes = {'Afrikaans': 'af', 'Albanian': 'sq', 'Amharic': 'am', 'Arabic': 'ar', 'Armenian': 'hy', 'Azerbaijani': 'az', 'Basque': 'eu', 'Belarusian': 'be', 'Bengali': 'bn', 'Bosnian': 'bs', 'Bulgarian': 'bg', 'Catalan': 'ca', 'Cebuano': 'ceb', 'Chinese (Simplified)': 'zh-CN', 'Chinese (Traditional)': 'zh-TW', 'Corsican': 'co', 'Croatian': 'hr', 'Czech': 'cs', 'Danish': 'da', 'Dutch': 'nl', 'English': 'en', 'Esperanto': 'eo', 'Estonian': 'et', 'Finnish': 'fi', 'French': 'fr', 'Frisian': 'fy', 'Galician': 'gl', 'Georgian': 'ka', 'German': 'de', 'Greek': 'el', 'Gujarati': 'gu', 'Haitian Creole': 'ht', 'Hausa': 'ha', 'Hawaiian': 'haw', 'Hebrew': 'iw', 'Hindi': 'hi', 'Hmong': 'hmn', 'Hungarian': 'hu', 'Icelandic': 'is', 'Igbo': 'ig', 'Indonesian': 'id', 'Irish': 'ga', 'Italian': 'it', 'Japanese': 'ja', 'Javanese': 'jw', 'Kannada': 'kn', 'Kazakh': 'kk', 'Khmer': 'km', 'Korean': 'ko', 'Kurdish': 'ku', 'Kyrgyz': 'ky', 'Lao': 'lo', 'Latin': 'la', 'Latvian': 'lv', 'Lithuanian': 'lt', 'Luxembourgish': 'lb', 'Macedonian': 'mk', 'Malagasy': 'mg', 'Malay': 'ms', 'Malayalam': 'ml', 'Maltese': 'mt', 'Maori': 'mi', 'Marathi': 'mr', 'Mongolian': 'mn', 'Myanmar (Burmese)': 'my', 'Nepali': 'ne', 'Norwegian': 'no', 'Nyanja (Chichewa)': 'ny', 'Pashto': 'ps', 'Persian': 'fa', 'Polish': 'pl', 'Portuguese (Portugal, Brazil)': 'pt', 'Punjabi': 'pa', 'Romanian': 'ro', 'Russian': 'ru', 'Samoan': 'sm', 'Scots Gaelic': 'gd', 'Serbian': 'sr', 'Sesotho': 'st', 'Shona': 'sn', 'Sindhi': 'sd', 'Sinhala (Sinhalese)': 'si', 'Slovak': 'sk', 'Slovenian': 'sl', 'Somali': 'so', 'Spanish': 'es', 'Sundanese': 'su', 'Swahili': 'sw', 'Swedish': 'sv', 'Tagalog (Filipino)': 'tl', 'Tajik': 'tg', 'Tamil': 'ta', 'Telugu': 'te', 'Thai': 'th', 'Turkish': 'tr', 'Ukrainian': 'uk', 'Urdu': 'ur', 'Uzbek': 'uz', 'Vietnamese': 'vi', 'Welsh': 'cy', 'Xhosa': 'xh', 'Yiddish': 'yi', 'Yoruba': 'yo', 'Zulu': 'zu'}\n",
        "overwrite_profile()\n",
        "\n",
        "\n",
        "print(\"\\033[1m \"+\"\\033[92m \")\n",
        "params = set()\n",
        "if \"Wizard-Vicuna-13B-Uncensored-GPTQ\" in model or \"4bit\" in model: params.add('--wbits 4 --groupsize 128 --model_type Llama') \n",
        "\n",
        "if load_in_8bit: params.add('--load-in-8bit')\n",
        "params.add('--share --chat') if not GetAPI else params.add('--public-api --chat')\n",
        "if not text_streaming or activate_google_translate: params.add('--no-stream')\n",
        "\n",
        "active_extensions = []\n",
        "if activate_sending_pictures: active_extensions.append('send_pictures')\n",
        "if activate_character_bias: active_extensions.append('character_bias')\n",
        "if kobold_memory: active_extensions.append('complex_memory')\n",
        "if activate_google_translate: active_extensions.append('google_translate')\n",
        "if GetAPI: active_extensions.append('api')\n",
        "if GrantBot_Access_to_Bing: active_extensions.append('EdgeGPT')\n",
        "\n",
        "active_extensions.append('gallery')\n",
        "\n",
        "if len(active_extensions) > 0: params.add(f'--extensions {\" \".join(active_extensions)}')\n",
        "\n",
        "# Starting the web UI\n",
        "if RunWebUI:\n",
        "  run_ooba()\n",
        "if not Debug: clear_output(); display(HTML(\"<img src='https://huggingface.co/Imablank/AnythingV3-1/resolve/main/bocchi-the-rock-bocchi.gif' width='300px'/>\"))\n",
        "\n",
        "print(\"\\033[1m \"+\"\\033[92m \")\n",
        "print(\"\\n[‚ö†Ô∏èTEXT-GEN-WEBUI SERVICE TERMINATED‚ö†Ô∏è]\")"
      ],
      "metadata": {
        "id": "qUJ8JBOB9WGi",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##**üöÄStart TextGenWebUIü™∂**\n",
        "from IPython.display import clear_output, display, HTML\n",
        "from IPython.utils import capture\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import os, sys, base64, subprocess, json, shutil, requests, time, pathlib\n",
        "\n",
        "#PARAMS\n",
        "#@markdown ####**üìñSetup Your UserProfile**\n",
        "\n",
        "UserName = \"\" #@param{type:'string'}\n",
        "Upload_ProfilePic = False #@param{type:'boolean'}\n",
        "save_logs_to_google_drive = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ######<font color=gray>*Save your data such as your name, logs, characters, etc.. in google drive*\n",
        "#@markdown ---\n",
        "#@markdown ####**‚öôÔ∏èConfigure and RunüöÄ**\n",
        "model = \"Pygmalion-350m(CPU)\" #@param [\"Pygmalion-13B-4bit\", \"Metharme-13B-4bit\", \"PygmalionCoT-7b\", \"Pygmalion-7B\", \"Metharme-7B\", \"Wizard-Vicuna-13B-Uncensored-GPTQ\", \"Pygmalion-350m(CPU)\", \"Pygmalion_6B_main_Sharded\", \"Pygmalion_6B_original_Sharded\", \"Pygmalion_6B_dev_Sharded\"]\n",
        "text_streaming = False #@param {type:\"boolean\"}\n",
        "activate_sending_pictures = False #@param {type:\"boolean\"}\n",
        "activate_character_bias = False #@param {type:\"boolean\"}\n",
        "\n",
        "GrantBot_Access_to_Bing = False #param {type:\"boolean\"}\n",
        "\n",
        "chat_language = \"English\" #@param ['Afrikaans', 'Albanian', 'Amharic', 'Arabic', 'Armenian', 'Azerbaijani', 'Basque', 'Belarusian', 'Bengali', 'Bosnian', 'Bulgarian', 'Catalan', 'Cebuano', 'Chinese (Simplified)', 'Chinese (Traditional)', 'Corsican', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English', 'Esperanto', 'Estonian', 'Finnish', 'French', 'Frisian', 'Galician', 'Georgian', 'German', 'Greek', 'Gujarati', 'Haitian Creole', 'Hausa', 'Hawaiian', 'Hebrew', 'Hindi', 'Hmong', 'Hungarian', 'Icelandic', 'Igbo', 'Indonesian', 'Irish', 'Italian', 'Japanese', 'Javanese', 'Kannada', 'Kazakh', 'Khmer', 'Korean', 'Kurdish', 'Kyrgyz', 'Lao', 'Latin', 'Latvian', 'Lithuanian', 'Luxembourgish', 'Macedonian', 'Malagasy', 'Malay', 'Malayalam', 'Maltese', 'Maori', 'Marathi', 'Mongolian', 'Myanmar (Burmese)', 'Nepali', 'Norwegian', 'Nyanja (Chichewa)', 'Pashto', 'Persian', 'Polish', 'Portuguese (Portugal, Brazil)', 'Punjabi', 'Romanian', 'Russian', 'Samoan', 'Scots Gaelic', 'Serbian', 'Sesotho', 'Shona', 'Sindhi', 'Sinhala (Sinhalese)', 'Slovak', 'Slovenian', 'Somali', 'Spanish', 'Sundanese', 'Swahili', 'Swedish', 'Tagalog (Filipino)', 'Tajik', 'Tamil', 'Telugu', 'Thai', 'Turkish', 'Ukrainian', 'Urdu', 'Uzbek', 'Vietnamese', 'Welsh', 'Xhosa', 'Yiddish', 'Yoruba', 'Zulu']\n",
        "activate_google_translate = (chat_language != \"English\")\n",
        "RunWebUI = True #@param {type:\"boolean\"}\n",
        "GetAPI = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=#eab676>Run SillyTavern instead of gradio or Get the oobabooga public API .\n",
        "Debug = True #@param{type:'boolean'}\n",
        "#@markdown > <font color=gray> *no bocchi? :(*\n",
        "\n",
        "#@markdown &nbsp; &nbsp;<img src=\"https://huggingface.co/Imablank/AnythingV3-1/resolve/main/ooba/bocchi-the-rock-crying.gif?size=90\" width=\"90\"/> \n",
        "\n",
        "dig=\"\"\"\n",
        "ot=\" \"\n",
        "load_in_8bit = True\n",
        "if \"Wizard-Vicuna-13B-Uncensored-GPTQ\" in model or \"4bit\" in model: load_in_8bit = False\n",
        "old_ooba = True\n",
        "kobold_memory = True\n",
        "\n",
        "#Main Directories:\n",
        "base_folder = \"/content/drive/MyDrive/oobabooga-data\"\n",
        "main_dir = \"/content/text-generation-webui/\"\n",
        "cache = f\"{main_dir}cache\"\n",
        "\n",
        "#PART 1 INSTALL TextGeN\n",
        "if save_logs_to_google_drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "def install_silly():\n",
        "  ForceInitSteps = []\n",
        "  class IncrementialInstall:\n",
        "    def __init__(self, root = \"/\", tasks = [], force = []):\n",
        "        self.tasks = tasks\n",
        "        self.path = os.path.join(root, \".ii\")\n",
        "        self.completed = list(filter(lambda x: not x in force, self.__completed()))\n",
        "\n",
        "    def __completed(self):\n",
        "        try:\n",
        "            with open(self.path) as f:\n",
        "                return json.load(f)\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "    def addTask(self, name, func):\n",
        "        self.tasks.append({\"name\": name, \"func\": func})\n",
        "\n",
        "    def run(self):\n",
        "        todo = list(filter(lambda x: not x[\"name\"] in self.completed, self.tasks))\n",
        "        try:\n",
        "            for task in todo:\n",
        "                task[\"func\"]()\n",
        "                self.completed.append(task[\"name\"])\n",
        "        finally:\n",
        "            with open(self.path, \"w\") as f:\n",
        "                json.dump(self.completed, f)\n",
        "\n",
        "  def create_paths(paths):\n",
        "    for directory in paths:\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "\n",
        "  def link(srcDir, destDir, files):\n",
        "    '''\n",
        "        Link source to dest copying dest to source if not present first\n",
        "    '''\n",
        "    for file in files:\n",
        "        source = os.path.join(srcDir, file)\n",
        "        dest = os.path.join(destDir, file)\n",
        "        if not os.path.exists(source):\n",
        "            !cp -r \"$dest\" \"$source\"\n",
        "        !rm -rf \"$dest\"\n",
        "        !ln -fs \"$source\" \"$dest\"\n",
        "  from google.colab import drive\n",
        "  if not save_logs_to_google_drive:\n",
        "    create_paths([\n",
        "            \"/content/SillyTavern-Data\"\n",
        "    ])\n",
        "  ii = IncrementialInstall(force=ForceInitSteps)\n",
        "# ---\n",
        "# SillyTavern py modules\n",
        "  def cloneTavern():\n",
        "    %cd /\n",
        "    !git clone https://github.com/Cohee1207/SillyTavern\n",
        "    !wget https://huggingface.co/Imablank/AnythingV3-1/resolve/main/ooba/globals.py && wget https://huggingface.co/Imablank/AnythingV3-1/resolve/main/ooba/extras_server.py \n",
        "  ii.addTask(\"Clone SillyTavern\", cloneTavern)\n",
        "  ii.run()\n",
        "  kargs = [\"/content/ckds\"]\n",
        "  kargs += [\"--localtunnel\", \"yes\"]\n",
        "\n",
        "# ---\n",
        "# nodejs\n",
        "  %cd /\n",
        "  def installNode():\n",
        "    !npm install -g n\n",
        "    !n 19\n",
        "    !node --version\n",
        "  installNode()\n",
        "# ---\n",
        "# TavernAI extras\n",
        "  import globals\n",
        "  globals.params = []\n",
        "  globals.params.append('--cpu')\n",
        "  %cd /SillyTavern\n",
        "  if save_logs_to_google_drive:\n",
        "    %env googledrive=2\n",
        "    def setupTavernPaths():\n",
        "        %cd /SillyTavern\n",
        "        tdrive = \"/content/drive/MyDrive/SillyTavern\"\n",
        "        create_paths([\n",
        "                tdrive,\n",
        "                os.path.join(\"public\", \"groups\"),\n",
        "                os.path.join(\"public\", \"group chats\")\n",
        "        ])\n",
        "        link(tdrive, \"public\", [\n",
        "                \"settings.json\",\n",
        "                \"backgrounds\",\n",
        "                \"characters\",\n",
        "                \"chats\",\n",
        "                \"User Avatars\",\n",
        "                \"worlds\",\n",
        "                \"group chats\",\n",
        "                \"groups\",\n",
        "        ])\n",
        "    ii.addTask(\"Setup Tavern Paths\", setupTavernPaths)\n",
        "  def installTavernDependencies():\n",
        "    %cd /SillyTavern\n",
        "    !npm install\n",
        "    !npm install -g localtunnel\n",
        "  ii.addTask(\"Install Tavern Dependencies\", installTavernDependencies)\n",
        "  ii.run()\n",
        "\n",
        "def universal_download(link1, path1, nan3):\n",
        "  !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {link1} -d {path1} -o {nan3}\n",
        "\n",
        "def edgegpt():\n",
        "  import zipfile\n",
        "  %cd /content/\n",
        "  !wget https://huggingface.co/Imablank/AnythingV3-1/resolve/main/ooba/EdgeGPT_working.zip\n",
        "  with zipfile.ZipFile('/content/EdgeGPT_working.zip', 'r') as zip:\n",
        "    zip.extractall(\"/content/text-generation-webui/extensions\")\n",
        "  !pip install -r {main_dir}extensions/EdgeGPT/requirements.txt && rm -rf /content/EdgeGPT_working.zip\n",
        "  %cd {main_dir}\n",
        "\n",
        "def install_ooba():\n",
        "  !apt-get -y install -qq aria2\n",
        "     \n",
        "  !mkdir {main_dir}cache\n",
        "  if save_logs_to_google_drive:\n",
        "    if not os.path.exists(f\"{base_folder}\"):\n",
        "      os.mkdir(f\"{base_folder}\")\n",
        "    if not os.path.exists(f\"{base_folder}/logs\"):\n",
        "      os.mkdir(f\"{base_folder}/logs\")\n",
        "    if not os.path.exists(f\"{base_folder}/User\"):\n",
        "      os.mkdir(f\"{base_folder}/User\")\n",
        "    if not os.path.exists(f\"{base_folder}/softprompts\"):\n",
        "      os.mkdir(f\"{base_folder}/softprompts\")\n",
        "    if not os.path.exists(f\"{base_folder}/characters\"):\n",
        "      shutil.move(\"text-generation-webui/characters\", f\"{base_folder}/characters\")\n",
        "    else:\n",
        "      !rm -r \"text-generation-webui/characters\"\n",
        "    \n",
        "    !rm -r \"text-generation-webui/softprompts\"\n",
        "    !ln -s \"$base_folder/logs\" \"text-generation-webui/\"\n",
        "    !ln -s \"$base_folder/softprompts\" \"text-generation-webui/softprompts\"\n",
        "    !ln -s \"$base_folder/characters\" \"text-generation-webui/characters\"\n",
        "    !ln -s \"$base_folder/User\" .\n",
        "  else:\n",
        "    !mkdir text-generation-webui/logs\n",
        "    \n",
        "  !ln -s text-generation-webui/logs .\n",
        "  !ln -s text-generation-webui/characters .\n",
        "  !ln -s text-generation-webui/models .\n",
        "    \n",
        "  %rm -r sample_data\n",
        "  %cd {main_dir}\n",
        "  !wget https://oobabooga.github.io/settings-colab.json -O settings-colab-template.json\n",
        "\n",
        "  !rm -rf \"/content/text-generation-webui/presets/Default.txt\" \"/content/text-generation-webui/settings-colab-template.json\" \"/content/text-generation-webui/settings-template.json\"\n",
        "\n",
        "  link=\"https://huggingface.co/Imablank/AnythingV3-1/raw/main/GPT4.txt, https://huggingface.co/Imablank/AnythingV3-1/raw/main/Alpha.json, https://huggingface.co/Imablank/AnythingV3-1/resolve/main/Alpha.png, https://huggingface.co/Imablank/AnythingV3-1/raw/main/settings.json\"\n",
        "  path=\"/content/text-generation-webui/presets/, /content/characters/,  /content/characters/, /content/text-generation-webui/\"\n",
        "  nan3=\"GPT4.txt, Alpha.json, Alpha.png, settings.json\"; link=link.split(', '); path=path.split(', '); nan3=nan3.split(', ')\n",
        "  for link, path, nan3 in zip(link, path, nan3):\n",
        "    universal_download(link, path, nan3)\n",
        "\n",
        "  %cd /content/text-generation-webui\n",
        "  !pip install -r requirements.txt\n",
        "  !pip install -r extensions/google_translate/requirements.txt\n",
        "  !pip install -r extensions/api/requirements.txt\n",
        "  \n",
        "  if \"Wizard-Vicuna-13B-Uncensored-GPTQ\" == model or \"4bit\" in model:\n",
        "    !mkdir /content/text-generation-webui/repositories\n",
        "    %cd repositories\n",
        "    !git clone -b v1.0 https://github.com/camenduru/GPTQ-for-LLaMa.git\n",
        "    %cd GPTQ-for-LLaMa\n",
        "    !python setup_cuda.py install\n",
        "\n",
        "if not os.path.exists(main_dir) or \"GPT4-x-alpaca/ROLEPLAY\" == model and not os.path.exists(\"/content/text-generation-webui/repositories\"):    \n",
        "  !cd /content \\\n",
        "    && git clone https://github.com/oobabooga/text-generation-webui \\\n",
        "    && git clone https://github.com/theubie/complex_memory {main_dir}/extensions/complex_memory     \n",
        "  \n",
        "  if old_ooba:\n",
        "    %cd {main_dir}\n",
        "    !git checkout f052ab9c8fed3dedc446c3847f10ab22e42bfb37\n",
        "\n",
        "  %cd /content\n",
        "  clear_output()\n",
        "\n",
        "  if not Debug:\n",
        "    display(HTML(\"<img src='https://huggingface.co/Imablank/AnythingV3-1/resolve/main/BoochiLoading.gif' width='280px'/>\"))\n",
        "    print(\"\\033[92m[Installing oobabooga]\")\n",
        "    with capture.capture_output() as cap:\n",
        "      install_ooba(); edgegpt()\n",
        "      if GetAPI: install_silly()\n",
        "  else:\n",
        "    install_ooba(); edgegpt()\n",
        "    if GetAPI: install_silly()\n",
        "  \n",
        "else: clear_output(); print(\"\\033[92m\\n[ ALREADY INSTALLED -- SKIPPING INSTALL.. ]\")\n",
        "\n",
        "!rm -rf \"/content/text-generation-webui/models/config.yaml\"\n",
        "\n",
        "#The Big boy's club\n",
        "def silly_tavern():\n",
        "  p = subprocess.Popen([\"lt\", \"--port\", \"5001\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "  Link=(p.stdout.readline().decode().strip())\n",
        "  # Get external IP address\n",
        "  external_ip = requests.get('https://api.ipify.org').text\n",
        "  external_ip=f\"\\n###Copy Google Colab's Endpoint IP###\\nEndpoint IP: \\033[4;92m{external_ip}\\033[0m\"\n",
        "  print(external_ip, f\"\\n###SillyTavern LINK###\\n{Link}\", sep=\"\\n\")\n",
        "  !node server.js\n",
        "\n",
        "def run_ooba(): \n",
        "  cmd = f\"python server.py --verbose --model {b} --settings {main_dir}cache/{_username_} {' '.join(params)}\"\n",
        "  print(cmd)\n",
        "  !$cmd\n",
        "  if os.path.exists(base_folder):\n",
        "    username_delete = os.listdir(\"/content/drive/MyDrive/oobabooga-data/User\")\n",
        "    try: username_delete.remove(\"pfp_me.png\")\n",
        "    except: pass\n",
        "    for username_delete in username_delete:\n",
        "      !rm -rf {base_folder}/User/$username_delete\n",
        "    if os.path.exists(\"/content/text-generation-webui/cache/pfp_me.png\"):\n",
        "      ot=\" /content/text-generation-webui/cache/pfp_me.png \"\n",
        "    !cp -r /content/text-generation-webui/cache/{_username_}{ot}/content/drive/MyDrive/oobabooga-data/User    \n",
        "\n",
        "\n",
        "def overwrite_profile():\n",
        "  global UserName, _username_\n",
        "  if not username_found == None and not \"\" == UserName:\n",
        "    del_userName= '[{'+username_found+\"}]-settings-colab.json\"\n",
        "    !rm -rf {cache}/{del_userName}   \n",
        "  elif not username_found == None and \"\" == UserName: _username_exists = \"[{\"+username_found+\"}]-settings-colab.json\"; UserName = username_found\n",
        "  elif username_found == None and \"\" == UserName: UserName=\"Anon-san\"\n",
        "  %cd {main_dir}\n",
        "  j = json.loads(open('settings.json', 'r').read())\n",
        "  j[\"google_translate-language string\"] = language_codes[chat_language]\n",
        "  j[\"name1\"] = UserName;\n",
        "  if \" \" in UserName: UserName=UserName.replace(\" \", \"_\")\n",
        "  _username_= '[{'+UserName+\"}]-settings-colab.json\"\n",
        "  with open(_username_, 'w') as f:\n",
        "    f.write(json.dumps(j, indent=4))\n",
        "  !rm -rf {main_dir}cache/{_username_exists}\n",
        "  !mv /content/text-generation-webui/{_username_} /content/text-generation-webui/cache\n",
        "  clear_output()\n",
        "\n",
        "def find_name():\n",
        "  if os.path.exists(base_folder+\"/User\"):\n",
        "    try:\n",
        "      user_profile = os.listdir(base_folder+\"/User\")\n",
        "      for count1 in user_profile:\n",
        "        !cp -r /content/drive/MyDrive/oobabooga-data/User/$count1 /content/text-generation-webui/cache\n",
        "    except: pass\n",
        "  try:\n",
        "    username_find_dir=os.listdir(\"/content/text-generation-webui/cache/\");\n",
        "    username_located=\", \".join(username_find_dir);\n",
        "    username_x1=username_located.index(\"[{\")+2;\n",
        "    username_x2=username_located.index(\"}]\");\n",
        "    username_found=username_located[username_x1:username_x2]\n",
        "  except: username_found = None\n",
        "  finally: return username_found\n",
        "\n",
        "def download_model(type):\n",
        "  tmp_repo=f\"/content/.{huggingface_repo}\"\n",
        "  !git lfs install --skip-smudge && export GIT_LFS_SKIP_SMUDGE=1 && git clone https://huggingface.co/{huggingface_org}/{huggingface_repo} /content/.{huggingface_repo} --branch {huggingface_branch}\n",
        "  !rm -rf {tmp_repo}/PygmalionCoT-7b-ggml-model-f16.bin {tmp_repo}/PygmalionCoT-7b-ggml-q4_2.bin {tmp_repo}/PygmalionCoT-7b-ggml-q5_1.bin {tmp_repo}/PygmalionCoT-7b-ggml-q8_0.bin {tmp_repo}/PygmalionCoT-7b-4bit-128g.safetensors {tmp_repo}/pytorch_model.bin.index.json {tmp_repo}/training_args.bin {tmp_repo}/trainer_state.json {tmp_repo}/all_results.json {tmp_repo}/eval_results.json {tmp_repo}/.gitattributes {tmp_repo}/train_results.json\n",
        "  repo = os.listdir(tmp_repo); clear_output()\n",
        "  for download in repo:    \n",
        "    link_path = f\"https://huggingface.co/{huggingface_org}/{huggingface_repo}/\"\n",
        "    if \".json\" in download or \".txt\" in download:\n",
        "      link_path += f\"raw/{huggingface_branch}/{download}\"\n",
        "    else: link_path += f\"resolve/{huggingface_branch}/{download}\"\n",
        "    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {link_path} -d /content/text-generation-webui/models/{type} -o {download}\n",
        "\n",
        "def upload_profile():  \n",
        "  #Path reader & png format converter\n",
        "  pfp = os.listdir(\"/.pfp/\");\n",
        "  for o in pfp: pfp=o.replace(\" \", \"_\"); pfp=\"/.pfp/\"+o\n",
        "  if os.path.exists(base_folder):\n",
        "    if \".png\" in o:\n",
        "      a = os.rename(pfp, \"pfp_me.png\")\n",
        "      !mv /.pfp/pfp_me.png /content/drive/MyDrive/oobabooga-data/User\n",
        "    else:\n",
        "      Ima1 = Image.open(r\"\"+pfp)\n",
        "      Ima1.save(r\"/content/drive/MyDrive/oobabooga-data/User/pfp_me.png\") #cache\n",
        "  else:\n",
        "    if \".png\" in o:\n",
        "      a = os.rename(pfp, \"pfp_me.png\")\n",
        "      !mv /.pfp/pfp_me.png /content/text-generation-webui/cache\n",
        "    else:\n",
        "      Ima1 = Image.open(r\"\"+pfp)\n",
        "      Ima1.save(r\"/content/text-generation-webui/cache/pfp_me.png\")\n",
        "\n",
        "#PART 2 DOWNLOAD AND START\n",
        "clear_output()\n",
        "!rm -rf /.pfp/ \\\n",
        "  && mkdir /.pfp/\n",
        "username_found = find_name()\n",
        "try:\n",
        "  if Upload_ProfilePic:\n",
        "    %cd /.pfp/\n",
        "    print(\"\\033[92m[Upload Your Profile Picture]\\n\");files.upload();\n",
        "    upload_profile()\n",
        "except: pass\n",
        "\n",
        "# DOWNLOAD\n",
        "%cd {main_dir}\n",
        "if \"Pygmalion-350m(CPU)\" in model:\n",
        "  huggingface_org=\"alpindale\";huggingface_repo=\"pygm-350m-experimental\";huggingface_branch=\"main\"\n",
        "elif \"PygmalionCoT-7b\" in model:\n",
        "  huggingface_org=\"notstoic\";huggingface_repo=\"PygmalionCoT-7b\";huggingface_branch=\"main\"\n",
        "elif \"Wizard-Vicuna-13B-Uncensored-GPTQ\" in model:\n",
        "  huggingface_org=\"TheBloke\";huggingface_repo=\"Wizard-Vicuna-13B-Uncensored-GPTQ\";huggingface_branch= \"main\"\n",
        "elif \"Pygmalion_6B_main_Sharded\" in model:\n",
        "  huggingface_org=\"waifu-workshop\";huggingface_repo=\"pygmalion-6b\";huggingface_branch=\"sharded\"\n",
        "elif \"Pygmalion_6B_original_Sharded\" in model:\n",
        "  huggingface_org=\"waifu-workshop\";huggingface_repo=\"pygmalion-6b\";huggingface_branch=\"original-sharded\"\n",
        "elif \"Pygmalion_6B_dev_Sharded\" in model:\n",
        "  huggingface_org=\"waifu-workshop\";huggingface_repo=\"pygmalion-6b\";huggingface_branch=\"dev-sharded\"\n",
        "elif \"Pygmalion-7B\" in model:\n",
        "  huggingface_org=\"TehVenom\";huggingface_repo=\"Pygmalion-7b-Merged-Safetensors\";huggingface_branch=\"main\"\n",
        "elif \"Metharme-7B\" in model:\n",
        "  huggingface_org=\"Imablank\";huggingface_repo=\"Metharme-7B-MERGED_WEIGHTS\";huggingface_branch=\"main\"\n",
        "elif \"Pygmalion-13B-4bit\" in model:\n",
        "  huggingface_org=\"notstoic\";huggingface_repo=\"pygmalion-13b-4bit-128g\";huggingface_branch=\"main\"\n",
        "elif \"Metharme-13B-4bit\" in model:\n",
        "  huggingface_org=\"TehVenom\";huggingface_repo=\"Metharme-13b-Merged\";huggingface_branch=\"main\"\n",
        "\n",
        "\n",
        "if not \"main\" == huggingface_branch: b = huggingface_repo+'_'+huggingface_branch\n",
        "else: b = huggingface_repo\n",
        "\n",
        "if not os.path.exists(\"/content/models/\"+huggingface_repo): download_model(b); clear_output(); print(\"\\033[96m\\n[\"+model+\" Installed]\\n\")\n",
        "\n",
        "clear_output()\n",
        "\n",
        "language_codes = {'Afrikaans': 'af', 'Albanian': 'sq', 'Amharic': 'am', 'Arabic': 'ar', 'Armenian': 'hy', 'Azerbaijani': 'az', 'Basque': 'eu', 'Belarusian': 'be', 'Bengali': 'bn', 'Bosnian': 'bs', 'Bulgarian': 'bg', 'Catalan': 'ca', 'Cebuano': 'ceb', 'Chinese (Simplified)': 'zh-CN', 'Chinese (Traditional)': 'zh-TW', 'Corsican': 'co', 'Croatian': 'hr', 'Czech': 'cs', 'Danish': 'da', 'Dutch': 'nl', 'English': 'en', 'Esperanto': 'eo', 'Estonian': 'et', 'Finnish': 'fi', 'French': 'fr', 'Frisian': 'fy', 'Galician': 'gl', 'Georgian': 'ka', 'German': 'de', 'Greek': 'el', 'Gujarati': 'gu', 'Haitian Creole': 'ht', 'Hausa': 'ha', 'Hawaiian': 'haw', 'Hebrew': 'iw', 'Hindi': 'hi', 'Hmong': 'hmn', 'Hungarian': 'hu', 'Icelandic': 'is', 'Igbo': 'ig', 'Indonesian': 'id', 'Irish': 'ga', 'Italian': 'it', 'Japanese': 'ja', 'Javanese': 'jw', 'Kannada': 'kn', 'Kazakh': 'kk', 'Khmer': 'km', 'Korean': 'ko', 'Kurdish': 'ku', 'Kyrgyz': 'ky', 'Lao': 'lo', 'Latin': 'la', 'Latvian': 'lv', 'Lithuanian': 'lt', 'Luxembourgish': 'lb', 'Macedonian': 'mk', 'Malagasy': 'mg', 'Malay': 'ms', 'Malayalam': 'ml', 'Maltese': 'mt', 'Maori': 'mi', 'Marathi': 'mr', 'Mongolian': 'mn', 'Myanmar (Burmese)': 'my', 'Nepali': 'ne', 'Norwegian': 'no', 'Nyanja (Chichewa)': 'ny', 'Pashto': 'ps', 'Persian': 'fa', 'Polish': 'pl', 'Portuguese (Portugal, Brazil)': 'pt', 'Punjabi': 'pa', 'Romanian': 'ro', 'Russian': 'ru', 'Samoan': 'sm', 'Scots Gaelic': 'gd', 'Serbian': 'sr', 'Sesotho': 'st', 'Shona': 'sn', 'Sindhi': 'sd', 'Sinhala (Sinhalese)': 'si', 'Slovak': 'sk', 'Slovenian': 'sl', 'Somali': 'so', 'Spanish': 'es', 'Sundanese': 'su', 'Swahili': 'sw', 'Swedish': 'sv', 'Tagalog (Filipino)': 'tl', 'Tajik': 'tg', 'Tamil': 'ta', 'Telugu': 'te', 'Thai': 'th', 'Turkish': 'tr', 'Ukrainian': 'uk', 'Urdu': 'ur', 'Uzbek': 'uz', 'Vietnamese': 'vi', 'Welsh': 'cy', 'Xhosa': 'xh', 'Yiddish': 'yi', 'Yoruba': 'yo', 'Zulu': 'zu'}\n",
        "overwrite_profile()\n",
        "\n",
        "\n",
        "print(\"\\033[1m \"+\"\\033[92m \")\n",
        "params = set()\n",
        "if \"Wizard-Vicuna-13B-Uncensored-GPTQ\" in model or \"4bit\" in model: params.add('--wbits 4 --groupsize 128 --model_type Llama') \n",
        "\n",
        "if load_in_8bit: params.add('--load-in-8bit')\n",
        "params.add('--share --chat') if not GetAPI else params.add('--public-api --chat')\n",
        "if not text_streaming or activate_google_translate: params.add('--no-stream')\n",
        "\n",
        "active_extensions = []\n",
        "if activate_sending_pictures: active_extensions.append('send_pictures')\n",
        "if activate_character_bias: active_extensions.append('character_bias')\n",
        "if kobold_memory: active_extensions.append('complex_memory')\n",
        "if activate_google_translate: active_extensions.append('google_translate')\n",
        "if GetAPI: active_extensions.append('api')\n",
        "if GrantBot_Access_to_Bing: active_extensions.append('EdgeGPT')\n",
        "\n",
        "active_extensions.append('gallery')\n",
        "\n",
        "if len(active_extensions) > 0: params.add(f'--extensions {\" \".join(active_extensions)}')\n",
        "\n",
        "# Starting the web UI\n",
        "if RunWebUI:\n",
        "  run_ooba()\n",
        "if not Debug: clear_output(); display(HTML(\"<img src='https://huggingface.co/Imablank/AnythingV3-1/resolve/main/bocchi-the-rock-bocchi.gif' width='300px'/>\"))\n",
        "\n",
        "print(\"\\033[1m \"+\"\\033[92m \")\n",
        "print(\"\\n[‚ö†Ô∏èTEXT-GEN-WEBUI SERVICE TERMINATED‚ö†Ô∏è]\")\n",
        "\"\"\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "VXKia4YX3EGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##**üöÄStart TextGenWebUI**\n",
        "#GOOGLE COLAB ONLY\n",
        "import sys\n",
        "sys.path.append(\"/\")\n",
        "%cd /\n",
        "!wget https://huggingface.co/Imablank/AnythingV3-1/resolve/main/ooba/Run_WebUI.py\n",
        "%cd -\n",
        "\n",
        "from Run_WebUI import *\n",
        "\n",
        "from IPython.display import clear_output, display, HTML\n",
        "from IPython.utils import capture\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import os, sys, base64, subprocess, json, shutil, requests, time, pathlib\n",
        "\n",
        "#PARAMS\n",
        "#@markdown ####**üìñSetup Your UserProfile**\n",
        "\n",
        "UserName = \"\" #@param{type:'string'}\n",
        "Upload_ProfilePic = False #@param{type:'boolean'}\n",
        "save_logs_to_google_drive = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ######<font color=gray>*Save your data such as your name, logs, characters, etc.. in google drive*\n",
        "#@markdown ---\n",
        "#@markdown ####**‚öôÔ∏èConfigure and RunüöÄ**\n",
        "model = \"Pygmalion-350m(CPU)\" #@param [\"Pygmalion-13B-4bit\", \"Metharme-13B-4bit\", \"PygmalionCoT-7b\", \"Pygmalion-7B\", \"Metharme-7B\", \"Wizard-Vicuna-13B-Uncensored-GPTQ\", \"Pygmalion-350m(CPU)\", \"Pygmalion_6B_main_Sharded\", \"Pygmalion_6B_original_Sharded\", \"Pygmalion_6B_dev_Sharded\"]\n",
        "text_streaming = False #@param {type:\"boolean\"}\n",
        "activate_sending_pictures = False #@param {type:\"boolean\"}\n",
        "activate_character_bias = False #@param {type:\"boolean\"}\n",
        "\n",
        "GrantBot_Access_to_Bing = False #param {type:\"boolean\"}\n",
        "\n",
        "chat_language = \"English\" #@param ['Afrikaans', 'Albanian', 'Amharic', 'Arabic', 'Armenian', 'Azerbaijani', 'Basque', 'Belarusian', 'Bengali', 'Bosnian', 'Bulgarian', 'Catalan', 'Cebuano', 'Chinese (Simplified)', 'Chinese (Traditional)', 'Corsican', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English', 'Esperanto', 'Estonian', 'Finnish', 'French', 'Frisian', 'Galician', 'Georgian', 'German', 'Greek', 'Gujarati', 'Haitian Creole', 'Hausa', 'Hawaiian', 'Hebrew', 'Hindi', 'Hmong', 'Hungarian', 'Icelandic', 'Igbo', 'Indonesian', 'Irish', 'Italian', 'Japanese', 'Javanese', 'Kannada', 'Kazakh', 'Khmer', 'Korean', 'Kurdish', 'Kyrgyz', 'Lao', 'Latin', 'Latvian', 'Lithuanian', 'Luxembourgish', 'Macedonian', 'Malagasy', 'Malay', 'Malayalam', 'Maltese', 'Maori', 'Marathi', 'Mongolian', 'Myanmar (Burmese)', 'Nepali', 'Norwegian', 'Nyanja (Chichewa)', 'Pashto', 'Persian', 'Polish', 'Portuguese (Portugal, Brazil)', 'Punjabi', 'Romanian', 'Russian', 'Samoan', 'Scots Gaelic', 'Serbian', 'Sesotho', 'Shona', 'Sindhi', 'Sinhala (Sinhalese)', 'Slovak', 'Slovenian', 'Somali', 'Spanish', 'Sundanese', 'Swahili', 'Swedish', 'Tagalog (Filipino)', 'Tajik', 'Tamil', 'Telugu', 'Thai', 'Turkish', 'Ukrainian', 'Urdu', 'Uzbek', 'Vietnamese', 'Welsh', 'Xhosa', 'Yiddish', 'Yoruba', 'Zulu']\n",
        "activate_google_translate = (chat_language != \"English\")\n",
        "RunWebUI = True #@param {type:\"boolean\"}\n",
        "GetAPI = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=#eab676>Run SillyTavern instead of gradio or Get the oobabooga public API .\n",
        "Debug = True #@param{type:'boolean'}\n",
        "#@markdown > <font color=gray> *no bocchi? :(*\n",
        "\n",
        "#@markdown &nbsp; &nbsp;<img src=\"https://huggingface.co/Imablank/AnythingV3-1/resolve/main/ooba/bocchi-the-rock-crying.gif?size=90\" width=\"90\"/> \n",
        "\n",
        "if not Debug:\n",
        "  display(HTML(\"<img src='https://huggingface.co/Imablank/AnythingV3-1/resolve/main/BoochiLoading.gif' width='280px'/>\"))\n",
        "  print(\"\\033[92m[Installing oobabooga]\")\n",
        "  with capture.capture_output() as cap:\n",
        "    install_ooba(); edgegpt()\n",
        "    if GetAPI: install_silly()\n",
        "else:\n",
        "  install_ooba(); edgegpt()\n",
        "  if GetAPI: install_silly()\n",
        "\n",
        "\n",
        "# OOBA ONLY\n",
        "def upload_profile():  \n",
        "  #Path reader & png format converter\n",
        "  pfp = os.listdir(\"/.pfp/\");\n",
        "  for o in pfp: pfp=o.replace(\" \", \"_\"); pfp=f\"/.pfp/{o}\"\n",
        "  if os.path.exists(base_folder):\n",
        "    if \".png\" in o:\n",
        "      a = os.rename(pfp, \"pfp_me.png\")\n",
        "      os.system(f\"mv /.pfp/pfp_me.png /content/drive/MyDrive/oobabooga-data/User\")\n",
        "    else:\n",
        "      Ima1 = Image.open(r\"\"+pfp)\n",
        "      Ima1.save(r\"/content/drive/MyDrive/oobabooga-data/User/pfp_me.png\") #cache\n",
        "  else:\n",
        "    if \".png\" in o:\n",
        "      a = os.rename(pfp, \"pfp_me.png\")\n",
        "      os.system(f\"mv /.pfp/pfp_me.png /content/text-generation-webui/cache\")\n",
        "    else:\n",
        "      Ima1 = Image.open(r\"\"+pfp)\n",
        "      Ima1.save(r\"/content/text-generation-webui/cache/pfp_me.png\")\n",
        "\n",
        "clear_output()\n",
        "os.system(f\"rm -rf /.pfp/ && mkdir /.pfp/\")\n",
        "\n",
        "#Upload Profile pic in ooba\n",
        "try:\n",
        "  if Upload_ProfilePic:\n",
        "    %cd /.pfp/\n",
        "    print(\"\\033[92m[Upload Your Profile Picture]\\n\");files.upload();\n",
        "    \n",
        "    upload_profile()\n",
        "except: pass\n",
        "\n",
        "language_codes = {'Afrikaans': 'af', 'Albanian': 'sq', 'Amharic': 'am', 'Arabic': 'ar', 'Armenian': 'hy', 'Azerbaijani': 'az', 'Basque': 'eu', 'Belarusian': 'be', 'Bengali': 'bn', 'Bosnian': 'bs', 'Bulgarian': 'bg', 'Catalan': 'ca', 'Cebuano': 'ceb', 'Chinese (Simplified)': 'zh-CN', 'Chinese (Traditional)': 'zh-TW', 'Corsican': 'co', 'Croatian': 'hr', 'Czech': 'cs', 'Danish': 'da', 'Dutch': 'nl', 'English': 'en', 'Esperanto': 'eo', 'Estonian': 'et', 'Finnish': 'fi', 'French': 'fr', 'Frisian': 'fy', 'Galician': 'gl', 'Georgian': 'ka', 'German': 'de', 'Greek': 'el', 'Gujarati': 'gu', 'Haitian Creole': 'ht', 'Hausa': 'ha', 'Hawaiian': 'haw', 'Hebrew': 'iw', 'Hindi': 'hi', 'Hmong': 'hmn', 'Hungarian': 'hu', 'Icelandic': 'is', 'Igbo': 'ig', 'Indonesian': 'id', 'Irish': 'ga', 'Italian': 'it', 'Japanese': 'ja', 'Javanese': 'jw', 'Kannada': 'kn', 'Kazakh': 'kk', 'Khmer': 'km', 'Korean': 'ko', 'Kurdish': 'ku', 'Kyrgyz': 'ky', 'Lao': 'lo', 'Latin': 'la', 'Latvian': 'lv', 'Lithuanian': 'lt', 'Luxembourgish': 'lb', 'Macedonian': 'mk', 'Malagasy': 'mg', 'Malay': 'ms', 'Malayalam': 'ml', 'Maltese': 'mt', 'Maori': 'mi', 'Marathi': 'mr', 'Mongolian': 'mn', 'Myanmar (Burmese)': 'my', 'Nepali': 'ne', 'Norwegian': 'no', 'Nyanja (Chichewa)': 'ny', 'Pashto': 'ps', 'Persian': 'fa', 'Polish': 'pl', 'Portuguese (Portugal, Brazil)': 'pt', 'Punjabi': 'pa', 'Romanian': 'ro', 'Russian': 'ru', 'Samoan': 'sm', 'Scots Gaelic': 'gd', 'Serbian': 'sr', 'Sesotho': 'st', 'Shona': 'sn', 'Sindhi': 'sd', 'Sinhala (Sinhalese)': 'si', 'Slovak': 'sk', 'Slovenian': 'sl', 'Somali': 'so', 'Spanish': 'es', 'Sundanese': 'su', 'Swahili': 'sw', 'Swedish': 'sv', 'Tagalog (Filipino)': 'tl', 'Tajik': 'tg', 'Tamil': 'ta', 'Telugu': 'te', 'Thai': 'th', 'Turkish': 'tr', 'Ukrainian': 'uk', 'Urdu': 'ur', 'Uzbek': 'uz', 'Vietnamese': 'vi', 'Welsh': 'cy', 'Xhosa': 'xh', 'Yiddish': 'yi', 'Yoruba': 'yo', 'Zulu': 'zu'}\n",
        "\n",
        "\n",
        "username_found = oobabooga.find_name()\n",
        "oobabooga.overwrite_profile()\n",
        "\n",
        "print(\"\\033[1m \"+\"\\033[92m \")\n",
        "\n",
        "os.chdir(main_dir)\n",
        "if \"Pygmalion-350m(CPU)\" in model:\n",
        "  huggingface_org=\"alpindale\";huggingface_repo=\"pygm-350m-experimental\";huggingface_branch=\"main\"\n",
        "elif \"PygmalionCoT-7b\" in model:\n",
        "  huggingface_org=\"notstoic\";huggingface_repo=\"PygmalionCoT-7b\";huggingface_branch=\"main\"\n",
        "elif \"Wizard-Vicuna-13B-Uncensored-GPTQ\" in model:\n",
        "  huggingface_org=\"TheBloke\";huggingface_repo=\"Wizard-Vicuna-13B-Uncensored-GPTQ\";huggingface_branch= \"main\"\n",
        "elif \"Pygmalion_6B_main_Sharded\" in model:\n",
        "  huggingface_org=\"waifu-workshop\";huggingface_repo=\"pygmalion-6b\";huggingface_branch=\"sharded\"\n",
        "elif \"Pygmalion_6B_original_Sharded\" in model:\n",
        "  huggingface_org=\"waifu-workshop\";huggingface_repo=\"pygmalion-6b\";huggingface_branch=\"original-sharded\"\n",
        "elif \"Pygmalion_6B_dev_Sharded\" in model:\n",
        "  huggingface_org=\"waifu-workshop\";huggingface_repo=\"pygmalion-6b\";huggingface_branch=\"dev-sharded\"\n",
        "elif \"Pygmalion-7B\" in model:\n",
        "  huggingface_org=\"TehVenom\";huggingface_repo=\"Pygmalion-7b-Merged-Safetensors\";huggingface_branch=\"main\"\n",
        "elif \"Metharme-7B\" in model:\n",
        "  huggingface_org=\"Imablank\";huggingface_repo=\"Metharme-7B-MERGED_WEIGHTS\";huggingface_branch=\"main\"\n",
        "elif \"Pygmalion-13B-4bit\" in model:\n",
        "  huggingface_org=\"notstoic\";huggingface_repo=\"pygmalion-13b-4bit-128g\";huggingface_branch=\"main\"\n",
        "elif \"Metharme-13B-4bit\" in model:\n",
        "  huggingface_org=\"TehVenom\";huggingface_repo=\"Metharme-13b-Merged\";huggingface_branch=\"main\"\n",
        "\n",
        "if not \"main\" == huggingface_branch: b = huggingface_repo+'_'+huggingface_branch\n",
        "else: b = huggingface_repo\n",
        "\n",
        "if not os.path.exists(f\"/content/models/{huggingface_repo}\"):\n",
        "  download_model(b);\n",
        "  clear_output(); print(f\"\\033[96m\\n[{model} Installed]\\n\")\n",
        "\n",
        "\n",
        "\n",
        "clear_output()\n",
        "\n",
        "\n",
        "# Starting the web UI\n",
        "if RunWebUI:\n",
        "  #run_silly_tavern()\n",
        "  run_ooba()\n",
        "if not Debug: clear_output(); display(HTML(\"<img src='https://huggingface.co/Imablank/AnythingV3-1/resolve/main/bocchi-the-rock-bocchi.gif' width='300px'/>\"))\n",
        "\n",
        "print(\"\\033[1m \"+\"\\033[92m \")\n",
        "print(\"\\n[TEXT-GEN-WEBUI SERVICE TERMINATED]\")\n",
        "\n"
      ],
      "metadata": {
        "id": "T6oyrr4X0wc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Receive\n",
        "\"\"\"\n",
        "var_list=sys.argv[1]\n",
        "\n",
        "#Main Variables\n",
        "\n",
        "UserName=var_list[0]\n",
        "save_logs_to_google_drive=var_list[1]\n",
        "model=var_list[2]\n",
        "text_streaming[3]\n",
        "activate_sending_pictures=var_list[4]\n",
        "activate_character_bias=var_list[5]\n",
        "chat_language=var_list[6]\n",
        "RunWebUI=var_list[7]\n",
        "GetAPI=var_list[8]\n",
        "Debug=var_list[9]\n",
        "\n",
        "activate_google_translate = (chat_language != \"English\")\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "ACCESS TOOLS\n",
        "\n",
        "IMPORTANT CLASS AND FUNCTIONs:\n",
        "\n",
        "def universal_download(link1, path1, nan3) = DOWNLOAD EVERYTHING\n",
        "def install_silly() = EVERYTHING SILLY\n",
        "class oobabooga = EVERYTHING OOBA\n",
        "class utils = MAIN CLASS TOOLS\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#PART 1 INSTALL \n",
        "\n",
        "def universal_download(link1, path1, nan3):\n",
        "  os.system(f\"aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {link1} -d {path1} -o {nan3}\")\n",
        "\n",
        "#SillyTavern\n",
        "def install_silly():\n",
        "  ForceInitSteps = []\n",
        "  class IncrementialInstall:\n",
        "    def __init__(self, root = \"/\", tasks = [], force = []):\n",
        "        self.tasks = tasks\n",
        "        self.path = os.path.join(root, \".ii\")\n",
        "        self.completed = list(filter(lambda x: not x in force, self.__completed()))\n",
        "    def __completed(self):\n",
        "        try:\n",
        "            with open(self.path) as f:\n",
        "                return json.load(f)\n",
        "        except:\n",
        "            return []\n",
        "    def addTask(self, name, func):\n",
        "        self.tasks.append({\"name\": name, \"func\": func})\n",
        "    def run(self):\n",
        "        todo = list(filter(lambda x: not x[\"name\"] in self.completed, self.tasks))\n",
        "        try:\n",
        "            for task in todo:\n",
        "                task[\"func\"]()\n",
        "                self.completed.append(task[\"name\"])\n",
        "        finally:\n",
        "            with open(self.path, \"w\") as f:\n",
        "                json.dump(self.completed, f)\n",
        "  def create_paths(paths):\n",
        "    for directory in paths:\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "  def link(srcDir, destDir, files):\n",
        "    '''\n",
        "        Link source to dest copying dest to source if not present first\n",
        "    '''\n",
        "    for file in files:\n",
        "        source = os.path.join(srcDir, file)\n",
        "        dest = os.path.join(destDir, file)\n",
        "        if not os.path.exists(source):\n",
        "            os.system(f\"cp -r {dest} {source}\")\n",
        "        os.system(f\"rm -rf {dest}\")\n",
        "        os.system(f\"ln -fs {source} {dest}\")\n",
        "  from google.colab import drive\n",
        "  if not save_logs_to_google_drive:\n",
        "    create_paths([\n",
        "            \"/content/SillyTavern-Data\"\n",
        "    ])\n",
        "  ii = IncrementialInstall(force=ForceInitSteps)\n",
        "# ---\n",
        "# SillyTavern py modules\n",
        "  def cloneTavern():\n",
        "    os.chdir('/')\n",
        "    os.system(f\"git clone https://github.com/Cohee1207/SillyTavern\")\n",
        "    os.system(f\"wget https://huggingface.co/Imablank/AnythingV3-1/resolve/main/ooba/globals.py && wget https://huggingface.co/Imablank/AnythingV3-1/resolve/main/ooba/extras_server.py\")\n",
        "  ii.addTask(\"Clone SillyTavern\", cloneTavern)\n",
        "  ii.run()\n",
        "  kargs = [\"/content/ckds\"]\n",
        "  kargs += [\"--localtunnel\", \"yes\"]\n",
        "\n",
        "# ---\n",
        "# nodejs\n",
        "  os.chdir('/')\n",
        "  def installNode():\n",
        "    os.system(f\"npm install -g n\")\n",
        "    os.system(f\"n 19\")\n",
        "    os.system(f\"node --version\")\n",
        "  installNode()\n",
        "# ---\n",
        "# TavernAI extras\n",
        "  import globals\n",
        "  globals.params = []\n",
        "  globals.params.append('--cpu')\n",
        "  os.chdir('/SillyTavern') \n",
        "  if save_logs_to_google_drive:\n",
        "    os.system(\"env googledrive=2\") #%env googledrive=2\n",
        "    def setupTavernPaths():\n",
        "        os.chdir('/SillyTavern') \n",
        "        tdrive = \"/content/drive/MyDrive/SillyTavern\"\n",
        "        create_paths([\n",
        "                tdrive,\n",
        "                os.path.join(\"public\", \"groups\"),\n",
        "                os.path.join(\"public\", \"group chats\")\n",
        "        ])\n",
        "        link(tdrive, \"public\", [\n",
        "                \"settings.json\",\n",
        "                \"backgrounds\",\n",
        "                \"characters\",\n",
        "                \"chats\",\n",
        "                \"User Avatars\",\n",
        "                \"worlds\",\n",
        "                \"group chats\",\n",
        "                \"groups\",\n",
        "        ])\n",
        "    ii.addTask(\"Setup Tavern Paths\", setupTavernPaths)\n",
        "  def installTavernDependencies():\n",
        "    os.chdir('/SillyTavern') \n",
        "    os.system(\"npm install\")\n",
        "    os.system(\"npm install -g localtunnel\")\n",
        "  ii.addTask(\"Install Tavern Dependencies\", installTavernDependencies)\n",
        "  ii.run()\n",
        "\n",
        "#Oobabooga\n",
        "\n",
        "class oobabooga:\n",
        "  global quant_, base_folder, main_dir, cache\n",
        "  #Main Directories:\n",
        "  base_folder = \"/content/drive/MyDrive/oobabooga-data\"\n",
        "  main_dir = \"/content/text-generation-webui/\"\n",
        "  cache = f\"{main_dir}cache\"\n",
        "\n",
        "  quant_=\"GPTQ|4bit|128\"\n",
        "  GrantBot_Access_to_Bing = False\n",
        "  load_in_8bit = True\n",
        "  if (model in quant_): load_in_8bit = False\n",
        "  old_ooba = True\n",
        "  complex_memory = True\n",
        "  filler=\" \"\n",
        "\n",
        "  def edgegpt():\n",
        "    import zipfile\n",
        "    os.chdir(\"/content/\")\n",
        "    os.system(f\"wget https://huggingface.co/Imablank/AnythingV3-1/resolve/main/ooba/EdgeGPT_working.zip\")\n",
        "    with zipfile.ZipFile('/content/EdgeGPT_working.zip', 'r') as zip:\n",
        "        zip.extractall(\"/content/text-generation-webui/extensions\")\n",
        "    os.system(f\"pip install -r {main_dir}extensions/EdgeGPT/requirements.txt && rm -rf /content/EdgeGPT_working.zip\")\n",
        "    os.chdir(main_dir)\n",
        "    \n",
        "  def overwrite_profile():\n",
        "    global UserName, _username_, base_folder, main_dir, cache\n",
        "    if not username_found == None and not \"\" == UserName:\n",
        "        del_userName= '[{'+username_found+\"}]-settings-colab.json\"\n",
        "        os.system(f\"rm -rf {cache}/{del_userName}\")    \n",
        "    elif not username_found == None and \"\" == UserName: _username_exists = \"[{\"+username_found+\"}]-settings-colab.json\"; UserName = username_found\n",
        "    elif username_found == None and \"\" == UserName: UserName=\"Anon-san\"\n",
        "    os.chdir(main_dir)  \n",
        "    j = json.loads(open('settings.json', 'r').read())\n",
        "    j[\"google_translate-language string\"] = language_codes[chat_language]\n",
        "    j[\"name1\"] = UserName;\n",
        "    if \" \" in UserName: UserName=UserName.replace(\" \", \"_\")\n",
        "    _username_= '[{'+UserName+\"}]-settings-colab.json\"\n",
        "    with open(_username_, 'w') as f:\n",
        "        f.write(json.dumps(j, indent=4))\n",
        "    os.system(f\"rm -rf {main_dir}cache/{_username_exists}\")\n",
        "    os.system(f\"mv /content/text-generation-webui/{_username_} /content/text-generation-webui/cache\")\n",
        "    clear_output()\n",
        "\n",
        "  def find_name():\n",
        "    global base_folder\n",
        "    if os.path.exists(base_folder+\"/User\"):\n",
        "        try:\n",
        "            user_profile = os.listdir(base_folder+\"/User\")\n",
        "            for count1 in user_profile:\n",
        "                os.system(f\"cp -r /content/drive/MyDrive/oobabooga-data/User/{count1} /content/text-generation-webui/cache\")\n",
        "        except: pass\n",
        "    try:\n",
        "        username_find_dir=os.listdir(\"/content/text-generation-webui/cache/\");\n",
        "        username_located=\", \".join(username_find_dir);\n",
        "        username_x1=username_located.index(\"[{\")+2;\n",
        "        username_x2=username_located.index(\"}]\");\n",
        "        username_found=username_located[username_x1:username_x2]\n",
        "    except: username_found = None\n",
        "    finally: return username_found\n",
        "\n",
        "  def install_ooba():\n",
        "    global quant_, base_folder, main_dir, cache\n",
        "    os.system(f\"apt-get -y install -qq aria2 && mkdir {main_dir}cache\")\n",
        "    if save_logs_to_google_drive:\n",
        "        if not os.path.exists(f\"{base_folder}\"):\n",
        "            os.mkdir(f\"{base_folder}\")\n",
        "        if not os.path.exists(f\"{base_folder}/logs\"):\n",
        "            os.mkdir(f\"{base_folder}/logs\")\n",
        "        if not os.path.exists(f\"{base_folder}/User\"):\n",
        "            os.mkdir(f\"{base_folder}/User\")\n",
        "        if not os.path.exists(f\"{base_folder}/softprompts\"):\n",
        "            os.mkdir(f\"{base_folder}/softprompts\")\n",
        "        if not os.path.exists(f\"{base_folder}/characters\"):\n",
        "            shutil.move(\"text-generation-webui/characters\", f\"{base_folder}/characters\")\n",
        "        else:\n",
        "            os.system(f\"rm -rf {main_dir}characters\")\n",
        "        \n",
        "        os.system(f\"rm -r text-generation-webui/softprompts\")\n",
        "        os.system(f\"ln -s $base_folder/logs text-generation-webui/\")\n",
        "        os.system(f\"ln -s $base_folder/softprompts text-generation-webui/softprompts\")\n",
        "        os.system(f\"ln -s $base_folder/characters text-generation-webui/characters\")\n",
        "        os.system(f\"ln -s $base_folder/User .\")\n",
        "    else:\n",
        "        os.system(f\"mkdir text-generation-webui/logs\")\n",
        "        \n",
        "    os.system(f\"ln -s text-generation-webui/logs .\")\n",
        "    os.system(f\"ln -s text-generation-webui/characters .\")\n",
        "    os.system(f\"ln -s text-generation-webui/models .\")\n",
        "        \n",
        "    os.system(\"rm -r sample_data\")\n",
        "    \n",
        "    os.chdir(main_dir)\n",
        "    os.system(f\"wget https://oobabooga.github.io/settings-colab.json -O settings-colab-template.json\")\n",
        "    os.system(f\"rm -rf /content/text-generation-webui/presets/Default.txt /content/text-generation-webui/settings-colab-template.json /content/text-generation-webui/settings-template.json\")\n",
        "\n",
        "    link=\"https://huggingface.co/Imablank/AnythingV3-1/raw/main/GPT4.txt, https://huggingface.co/Imablank/AnythingV3-1/raw/main/Alpha.json, https://huggingface.co/Imablank/AnythingV3-1/resolve/main/Alpha.png, https://huggingface.co/Imablank/AnythingV3-1/raw/main/settings.json\"\n",
        "    path=\"/content/text-generation-webui/presets/, /content/characters/,    /content/characters/, /content/text-generation-webui/\"\n",
        "    nan3=\"GPT4.txt, Alpha.json, Alpha.png, settings.json\"; link=link.split(', '); path=path.split(', '); nan3=nan3.split(', ')\n",
        "    for link, path, nan3 in zip(link, path, nan3):\n",
        "        universal_download(link, path, nan3)\n",
        "\n",
        "    os.chdir(main_dir)\n",
        "    os.system(f\"pip install -r requirements.txt && pip install -r extensions/google_translate/requirements.txt && pip install -r extensions/api/requirements.txt\")\n",
        "    \n",
        "    if model in quant_:\n",
        "        os.system(f\"mkdir /content/text-generation-webui/repositories\")\n",
        "        os.chdir(\"/repositories/\")\n",
        "        os.system(f\"git clone -b v1.0 https://github.com/camenduru/GPTQ-for-LLaMa.git\")\n",
        "        os.chdir(\"/GPTQ-for-LLaMa/\")\n",
        "        os.system(f\"python setup_cuda.py install\")\n",
        "\n",
        "  def prepare_ooba():\n",
        "    if not os.path.exists(main_dir) or model in quant_ and not os.path.exists(\"/content/text-generation-webui/repositories\"):\n",
        "      os.chdir(\"/content\")\n",
        "      os.system(f\"git clone https://github.com/oobabooga/text-generation-webui && git clone https://github.com/theubie/complex_memory {main_dir}/extensions/complex_memory\")\n",
        "    \n",
        "      if old_ooba: os.chdir(main_dir); os.system(f\"git checkout f052ab9c8fed3dedc446c3847f10ab22e42bfb37\")\n",
        "      install_ooba()\n",
        "\n",
        "    else: clear_output(); print(\"\\033[92m\\n[ ALREADY INSTALLED -- SKIPPING INSTALL.. ]\")\n",
        "    os.system(f\"rm -rf /content/text-generation-webui/models/config.yaml\")\n",
        "    os.chdir(\"/content\")\n",
        "    clear_output()\n",
        "\n",
        "class utils:\n",
        "  from IPython.utils import capture\n",
        "  from google.colab import files\n",
        "  from PIL import Image\n",
        "  import os, sys, base64, subprocess, json, shutil, requests, time, pathlib\n",
        "\n",
        "  def install_everything(modes):\n",
        "    pass\n",
        "    #class oobabooga()\n",
        "    \"\"\"\n",
        "    .prepare_ooba(), install_ooba(), .edgegpt(), .overwrite_profile(), .find_name()\n",
        "    \"\"\"\n",
        "    #def install_silly():\n",
        "    #def universal_download(link1, path1, nan3):\n",
        "\n",
        "  def run_silly_tavern():\n",
        "    p = subprocess.Popen([\"lt\", \"--port\", \"5001\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    Link=(p.stdout.readline().decode().strip())\n",
        "    # Get external IP address\n",
        "    external_ip = requests.get('https://api.ipify.org').text\n",
        "    external_ip=f\"\\n###Copy Google Colab's Endpoint IP###\\nEndpoint IP: \\033[4;92m{external_ip}\\033[0m\"\n",
        "    print(external_ip, f\"\\n###SillyTavern LINK###\\n{Link}\", sep=\"\\n\")\n",
        "    os.system(f\"node server.js\")\n",
        "\n",
        "  def run_ooba():\n",
        "    global quant_\n",
        "    params = set()\n",
        "    if model in quant_: params.add('--wbits 4 --groupsize 128 --model_type Llama')\n",
        "  \n",
        "    if load_in_8bit: params.add('--load-in-8bit')\n",
        "    params.add('--share --chat') if not GetAPI else params.add('--public-api --chat')\n",
        "    if not text_streaming or activate_google_translate: params.add('--no-stream')\n",
        "    active_extensions = []\n",
        "    if activate_sending_pictures: active_extensions.append('send_pictures')\n",
        "    if activate_character_bias: active_extensions.append('character_bias')\n",
        "    if complex_memory: active_extensions.append('complex_memory')\n",
        "    if activate_google_translate: active_extensions.append('google_translate')\n",
        "    if GetAPI: active_extensions.append('api')\n",
        "    if GrantBot_Access_to_Bing: active_extensions.append('EdgeGPT')\n",
        "    active_extensions.append('gallery')\n",
        "    if len(active_extensions) > 0: params.add(f'--extensions {\" \".join(active_extensions)}')\n",
        "  \n",
        "    cmd = f\"python server.py --verbose --model {b} --settings {main_dir}cache/{_username_} {' '.join(params)}\"\n",
        "    print(cmd)\n",
        "    os.system(cmd)\n",
        "  \n",
        "    #Save to drive system\n",
        "    if os.path.exists(base_folder):\n",
        "      username_delete = os.listdir(\"/content/drive/MyDrive/oobabooga-data/User\")\n",
        "      try: username_delete.remove(\"pfp_me.png\")\n",
        "      except: pass\n",
        "      for username_delete in username_delete:\n",
        "        os.system(f\"rm -rf {base_folder}/User/{username_delete}\")\n",
        "      if os.path.exists(\"/content/text-generation-webui/cache/pfp_me.png\"):\n",
        "        filler=\" /content/text-generation-webui/cache/pfp_me.png \"\n",
        "      os.system(f\"cp -r /content/text-generation-webui/cache/{_username_}{filler}/content/drive/MyDrive/oobabooga-data/User\")\n",
        "\n",
        "  \n",
        "  def download_model(type):\n",
        "    tmp_repo=f\"/content/.{huggingface_repo}\"\n",
        "    os.system(f\"git lfs install --skip-smudge && export GIT_LFS_SKIP_SMUDGE=1 && git clone https://huggingface.co/{huggingface_org}/{huggingface_repo} /content/.{huggingface_repo} --branch {huggingface_branch}\")\n",
        "    os.system(f\"rm -rf  {tmp_repo}/PygmalionCoT-7b-ggml-model-f16.bin {tmp_repo}/PygmalionCoT-7b-ggml-q4_2.bin {tmp_repo}/PygmalionCoT-7b-ggml-q5_1.bin {tmp_repo}/PygmalionCoT-7b-ggml-q8_0.bin {tmp_repo}/PygmalionCoT-7b-4bit-128g.safetensors {tmp_repo}/pytorch_model.bin.index.json {tmp_repo}/training_args.bin {tmp_repo}/trainer_state.json {tmp_repo}/all_results.json {tmp_repo}/eval_results.json {tmp_repo}/.gitattributes {tmp_repo}/train_results.json\")\n",
        "    repo = os.listdir(tmp_repo); clear_output()\n",
        "    for download in repo:    \n",
        "      link_path = f\"https://huggingface.co/{huggingface_org}/{huggingface_repo}/\"\n",
        "      if \".json\" in download or \".txt\" in download:\n",
        "        link_path += f\"raw/{huggingface_branch}/{download}\"\n",
        "      else: link_path += f\"resolve/{huggingface_branch}/{download}\"\n",
        "      os.system(f\"aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {link_path} -d /content/text-generation-webui/models/{type} -o {download}\")\n"
      ],
      "metadata": {
        "id": "S1gXNWpfMumo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Default title text\n",
        "\n",
        "#Receive\n",
        "\"\"\"\n",
        "var_list=sys.argv[1]\n",
        "\n",
        "#Extract\n",
        "\n",
        "UserName=var_list[0]\n",
        "save_logs_to_google_drive=var_list[1]\n",
        "model=var_list[2]\n",
        "text_streaming[3]\n",
        "activate_sending_pictures=var_list[4]\n",
        "activate_character_bias=var_list[5]\n",
        "chat_language=var_list[6]\n",
        "RunWebUI=var_list[7]\n",
        "GetAPI=var_list[8]\n",
        "Debug=var_list[9]\n",
        "\n",
        "activate_google_translate = (chat_language != \"English\")\n",
        "\"\"\"\n",
        "\n",
        "#PART 1 INSTALL TextGeN\n",
        "\n",
        "def universal_download(link1, path1, nan3):\n",
        "  os.system(f\"aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {link1} -d {path1} -o {nan3}\")\n",
        "\n",
        "#SillyTavern\n",
        "def install_silly():\n",
        "  ForceInitSteps = []\n",
        "  class IncrementialInstall:\n",
        "    def __init__(self, root = \"/\", tasks = [], force = []):\n",
        "        self.tasks = tasks\n",
        "        self.path = os.path.join(root, \".ii\")\n",
        "        self.completed = list(filter(lambda x: not x in force, self.__completed()))\n",
        "    def __completed(self):\n",
        "        try:\n",
        "            with open(self.path) as f:\n",
        "                return json.load(f)\n",
        "        except:\n",
        "            return []\n",
        "    def addTask(self, name, func):\n",
        "        self.tasks.append({\"name\": name, \"func\": func})\n",
        "    def run(self):\n",
        "        todo = list(filter(lambda x: not x[\"name\"] in self.completed, self.tasks))\n",
        "        try:\n",
        "            for task in todo:\n",
        "                task[\"func\"]()\n",
        "                self.completed.append(task[\"name\"])\n",
        "        finally:\n",
        "            with open(self.path, \"w\") as f:\n",
        "                json.dump(self.completed, f)\n",
        "  def create_paths(paths):\n",
        "    for directory in paths:\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "  def link(srcDir, destDir, files):\n",
        "    '''\n",
        "        Link source to dest copying dest to source if not present first\n",
        "    '''\n",
        "    for file in files:\n",
        "        source = os.path.join(srcDir, file)\n",
        "        dest = os.path.join(destDir, file)\n",
        "        if not os.path.exists(source):\n",
        "            os.system(f\"cp -r {dest} {source}\")\n",
        "        os.system(f\"rm -rf {dest}\")\n",
        "        os.system(f\"ln -fs {source} {dest}\")\n",
        "  from google.colab import drive\n",
        "  if not save_logs_to_google_drive:\n",
        "    create_paths([\n",
        "            \"/content/SillyTavern-Data\"\n",
        "    ])\n",
        "  ii = IncrementialInstall(force=ForceInitSteps)\n",
        "# ---\n",
        "# SillyTavern py modules\n",
        "  def cloneTavern():\n",
        "    os.chdir('/')\n",
        "    os.system(f\"git clone https://github.com/Cohee1207/SillyTavern\")\n",
        "    os.system(f\"wget https://huggingface.co/Imablank/AnythingV3-1/resolve/main/ooba/globals.py && wget https://huggingface.co/Imablank/AnythingV3-1/resolve/main/ooba/extras_server.py\")\n",
        "  ii.addTask(\"Clone SillyTavern\", cloneTavern)\n",
        "  ii.run()\n",
        "  kargs = [\"/content/ckds\"]\n",
        "  kargs += [\"--localtunnel\", \"yes\"]\n",
        "\n",
        "# ---\n",
        "# nodejs\n",
        "  os.chdir('/')\n",
        "  def installNode():\n",
        "    os.system(f\"npm install -g n\")\n",
        "    os.system(f\"n 19\")\n",
        "    os.system(f\"node --version\")\n",
        "  installNode()\n",
        "# ---\n",
        "# TavernAI extras\n",
        "  import globals\n",
        "  globals.params = []\n",
        "  globals.params.append('--cpu')\n",
        "  os.chdir('/SillyTavern') \n",
        "  if save_logs_to_google_drive:\n",
        "    os.system(\"env googledrive=2\") #%env googledrive=2\n",
        "    def setupTavernPaths():\n",
        "        os.chdir('/SillyTavern') \n",
        "        tdrive = \"/content/drive/MyDrive/SillyTavern\"\n",
        "        create_paths([\n",
        "                tdrive,\n",
        "                os.path.join(\"public\", \"groups\"),\n",
        "                os.path.join(\"public\", \"group chats\")\n",
        "        ])\n",
        "        link(tdrive, \"public\", [\n",
        "                \"settings.json\",\n",
        "                \"backgrounds\",\n",
        "                \"characters\",\n",
        "                \"chats\",\n",
        "                \"User Avatars\",\n",
        "                \"worlds\",\n",
        "                \"group chats\",\n",
        "                \"groups\",\n",
        "        ])\n",
        "    ii.addTask(\"Setup Tavern Paths\", setupTavernPaths)\n",
        "  def installTavernDependencies():\n",
        "    os.chdir('/SillyTavern') \n",
        "    os.system(\"npm install\")\n",
        "    os.system(\"npm install -g localtunnel\")\n",
        "  ii.addTask(\"Install Tavern Dependencies\", installTavernDependencies)\n",
        "  ii.run()\n",
        "\n",
        "#Oobabooga\n",
        "\n",
        "class oobabooga:\n",
        "  global quant_, base_folder, main_dir, cache\n",
        "  #Main Directories:\n",
        "  base_folder = \"/content/drive/MyDrive/oobabooga-data\"\n",
        "  main_dir = \"/content/text-generation-webui/\"\n",
        "  cache = f\"{main_dir}cache\"\n",
        "\n",
        "  quant_=\"GPTQ|4bit|128\"\n",
        "  GrantBot_Access_to_Bing = False\n",
        "  load_in_8bit = True\n",
        "  if (model in quant_): load_in_8bit = False\n",
        "  old_ooba = True\n",
        "  complex_memory = True\n",
        "  filler=\" \"\n",
        "\n",
        "  def edgegpt():\n",
        "    import zipfile\n",
        "    os.chdir(\"/content/\")\n",
        "    os.system(f\"wget https://huggingface.co/Imablank/AnythingV3-1/resolve/main/ooba/EdgeGPT_working.zip\")\n",
        "    with zipfile.ZipFile('/content/EdgeGPT_working.zip', 'r') as zip:\n",
        "        zip.extractall(\"/content/text-generation-webui/extensions\")\n",
        "    os.system(f\"pip install -r {main_dir}extensions/EdgeGPT/requirements.txt && rm -rf /content/EdgeGPT_working.zip\")\n",
        "    os.chdir(main_dir)\n",
        "    \n",
        "  def overwrite_profile():\n",
        "    global UserName, _username_, base_folder, main_dir, cache\n",
        "    if not username_found == None and not \"\" == UserName:\n",
        "        del_userName= '[{'+username_found+\"}]-settings-colab.json\"\n",
        "        os.system(f\"rm -rf {cache}/{del_userName}\")    \n",
        "    elif not username_found == None and \"\" == UserName: _username_exists = \"[{\"+username_found+\"}]-settings-colab.json\"; UserName = username_found\n",
        "    elif username_found == None and \"\" == UserName: UserName=\"Anon-san\"\n",
        "    os.chdir(main_dir)  \n",
        "    j = json.loads(open('settings.json', 'r').read())\n",
        "    j[\"google_translate-language string\"] = language_codes[chat_language]\n",
        "    j[\"name1\"] = UserName;\n",
        "    if \" \" in UserName: UserName=UserName.replace(\" \", \"_\")\n",
        "    _username_= '[{'+UserName+\"}]-settings-colab.json\"\n",
        "    with open(_username_, 'w') as f:\n",
        "        f.write(json.dumps(j, indent=4))\n",
        "    os.system(f\"rm -rf {main_dir}cache/{_username_exists}\")\n",
        "    os.system(f\"mv /content/text-generation-webui/{_username_} /content/text-generation-webui/cache\")\n",
        "    clear_output()\n",
        "\n",
        "  def find_name():\n",
        "    global base_folder\n",
        "    if os.path.exists(base_folder+\"/User\"):\n",
        "        try:\n",
        "            user_profile = os.listdir(base_folder+\"/User\")\n",
        "            for count1 in user_profile:\n",
        "                os.system(f\"cp -r /content/drive/MyDrive/oobabooga-data/User/{count1} /content/text-generation-webui/cache\")\n",
        "        except: pass\n",
        "    try:\n",
        "        username_find_dir=os.listdir(\"/content/text-generation-webui/cache/\");\n",
        "        username_located=\", \".join(username_find_dir);\n",
        "        username_x1=username_located.index(\"[{\")+2;\n",
        "        username_x2=username_located.index(\"}]\");\n",
        "        username_found=username_located[username_x1:username_x2]\n",
        "    except: username_found = None\n",
        "    finally: return username_found\n",
        "\n",
        "  def install_ooba():\n",
        "    global quant_, base_folder, main_dir, cache\n",
        "    os.system(f\"apt-get -y install -qq aria2 && mkdir {main_dir}cache\")\n",
        "    if save_logs_to_google_drive:\n",
        "        if not os.path.exists(f\"{base_folder}\"):\n",
        "            os.mkdir(f\"{base_folder}\")\n",
        "        if not os.path.exists(f\"{base_folder}/logs\"):\n",
        "            os.mkdir(f\"{base_folder}/logs\")\n",
        "        if not os.path.exists(f\"{base_folder}/User\"):\n",
        "            os.mkdir(f\"{base_folder}/User\")\n",
        "        if not os.path.exists(f\"{base_folder}/softprompts\"):\n",
        "            os.mkdir(f\"{base_folder}/softprompts\")\n",
        "        if not os.path.exists(f\"{base_folder}/characters\"):\n",
        "            shutil.move(\"text-generation-webui/characters\", f\"{base_folder}/characters\")\n",
        "        else:\n",
        "            os.system(f\"rm -rf {main_dir}characters\")\n",
        "        \n",
        "        os.system(f\"rm -r text-generation-webui/softprompts\")\n",
        "        os.system(f\"ln -s $base_folder/logs text-generation-webui/\")\n",
        "        os.system(f\"ln -s $base_folder/softprompts text-generation-webui/softprompts\")\n",
        "        os.system(f\"ln -s $base_folder/characters text-generation-webui/characters\")\n",
        "        os.system(f\"ln -s $base_folder/User .\")\n",
        "    else:\n",
        "        os.system(f\"mkdir text-generation-webui/logs\")\n",
        "        \n",
        "    os.system(f\"ln -s text-generation-webui/logs .\")\n",
        "    os.system(f\"ln -s text-generation-webui/characters .\")\n",
        "    os.system(f\"ln -s text-generation-webui/models .\")\n",
        "        \n",
        "    os.system(\"rm -r sample_data\")\n",
        "    \n",
        "    os.chdir(main_dir)\n",
        "    os.system(f\"wget https://oobabooga.github.io/settings-colab.json -O settings-colab-template.json\")\n",
        "    os.system(f\"rm -rf /content/text-generation-webui/presets/Default.txt /content/text-generation-webui/settings-colab-template.json /content/text-generation-webui/settings-template.json\")\n",
        "\n",
        "    link=\"https://huggingface.co/Imablank/AnythingV3-1/raw/main/GPT4.txt, https://huggingface.co/Imablank/AnythingV3-1/raw/main/Alpha.json, https://huggingface.co/Imablank/AnythingV3-1/resolve/main/Alpha.png, https://huggingface.co/Imablank/AnythingV3-1/raw/main/settings.json\"\n",
        "    path=\"/content/text-generation-webui/presets/, /content/characters/,    /content/characters/, /content/text-generation-webui/\"\n",
        "    nan3=\"GPT4.txt, Alpha.json, Alpha.png, settings.json\"; link=link.split(', '); path=path.split(', '); nan3=nan3.split(', ')\n",
        "    for link, path, nan3 in zip(link, path, nan3):\n",
        "        universal_download(link, path, nan3)\n",
        "\n",
        "    os.chdir(main_dir)\n",
        "    os.system(f\"pip install -r requirements.txt && pip install -r extensions/google_translate/requirements.txt && pip install -r extensions/api/requirements.txt\")\n",
        "    \n",
        "    if model in quant_:\n",
        "        os.system(f\"mkdir /content/text-generation-webui/repositories\")\n",
        "        os.chdir(\"/repositories/\")\n",
        "        os.system(f\"git clone -b v1.0 https://github.com/camenduru/GPTQ-for-LLaMa.git\")\n",
        "        os.chdir(\"/GPTQ-for-LLaMa/\")\n",
        "        os.system(f\"python setup_cuda.py install\")\n",
        "\n",
        "  if not os.path.exists(main_dir) or model in quant_ and not os.path.exists(\"/content/text-generation-webui/repositories\"):\n",
        "    os.chdir(\"/content\")\n",
        "    os.system(f\"git clone https://github.com/oobabooga/text-generation-webui && git clone https://github.com/theubie/complex_memory {main_dir}/extensions/complex_memory\")\n",
        "    #install_ooba()\n",
        "    if old_ooba: os.chdir(main_dir); os.system(f\"git checkout f052ab9c8fed3dedc446c3847f10ab22e42bfb37\")\n",
        "  else: clear_output(); print(\"\\033[92m\\n[ ALREADY INSTALLED -- SKIPPING INSTALL.. ]\")\n",
        "  os.system(f\"rm -rf /content/text-generation-webui/models/config.yaml\")\n",
        "  os.chdir(\"/content\")\n",
        "  clear_output()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class utils:\n",
        "  from IPython.utils import capture\n",
        "  from google.colab import files\n",
        "  from PIL import Image\n",
        "  import os, sys, base64, subprocess, json, shutil, requests, time, pathlib\n",
        "  \n",
        "  def run_silly_tavern():\n",
        "    p = subprocess.Popen([\"lt\", \"--port\", \"5001\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    Link=(p.stdout.readline().decode().strip())\n",
        "    # Get external IP address\n",
        "    external_ip = requests.get('https://api.ipify.org').text\n",
        "    external_ip=f\"\\n###Copy Google Colab's Endpoint IP###\\nEndpoint IP: \\033[4;92m{external_ip}\\033[0m\"\n",
        "    print(external_ip, f\"\\n###SillyTavern LINK###\\n{Link}\", sep=\"\\n\")\n",
        "    os.system(f\"node server.js\")\n",
        "\n",
        "  def run_ooba():\n",
        "    params = set()\n",
        "    if \"Wizard-Vicuna-13B-Uncensored-GPTQ\" in model or \"4bit\" in model: params.add('--wbits 4 --groupsize 128 --model_type Llama')\n",
        "  \n",
        "    if load_in_8bit: params.add('--load-in-8bit')\n",
        "    params.add('--share --chat') if not GetAPI else params.add('--public-api --chat')\n",
        "    if not text_streaming or activate_google_translate: params.add('--no-stream')\n",
        "    active_extensions = []\n",
        "    if activate_sending_pictures: active_extensions.append('send_pictures')\n",
        "    if activate_character_bias: active_extensions.append('character_bias')\n",
        "    if complex_memory: active_extensions.append('complex_memory')\n",
        "    if activate_google_translate: active_extensions.append('google_translate')\n",
        "    if GetAPI: active_extensions.append('api')\n",
        "    if GrantBot_Access_to_Bing: active_extensions.append('EdgeGPT')\n",
        "    active_extensions.append('gallery')\n",
        "    if len(active_extensions) > 0: params.add(f'--extensions {\" \".join(active_extensions)}')\n",
        "  \n",
        "    cmd = f\"python server.py --verbose --model {b} --settings {main_dir}cache/{_username_} {' '.join(params)}\"\n",
        "    print(cmd)\n",
        "    os.system(cmd)\n",
        "  \n",
        "    #Save to drive system\n",
        "    if os.path.exists(base_folder):\n",
        "      username_delete = os.listdir(\"/content/drive/MyDrive/oobabooga-data/User\")\n",
        "      try: username_delete.remove(\"pfp_me.png\")\n",
        "      except: pass\n",
        "      for username_delete in username_delete:\n",
        "        os.system(f\"rm -rf {base_folder}/User/{username_delete}\")\n",
        "      if os.path.exists(\"/content/text-generation-webui/cache/pfp_me.png\"):\n",
        "        filler=\" /content/text-generation-webui/cache/pfp_me.png \"\n",
        "      os.system(f\"cp -r /content/text-generation-webui/cache/{_username_}{filler}/content/drive/MyDrive/oobabooga-data/User\")\n",
        "\n",
        "  def download_model(type):\n",
        "    tmp_repo=f\"/content/.{huggingface_repo}\"\n",
        "    os.system(f\"git lfs install --skip-smudge && export GIT_LFS_SKIP_SMUDGE=1 && git clone https://huggingface.co/{huggingface_org}/{huggingface_repo} /content/.{huggingface_repo} --branch {huggingface_branch}\")\n",
        "    os.system(f\"rm -rf  {tmp_repo}/PygmalionCoT-7b-ggml-model-f16.bin {tmp_repo}/PygmalionCoT-7b-ggml-q4_2.bin {tmp_repo}/PygmalionCoT-7b-ggml-q5_1.bin {tmp_repo}/PygmalionCoT-7b-ggml-q8_0.bin {tmp_repo}/PygmalionCoT-7b-4bit-128g.safetensors {tmp_repo}/pytorch_model.bin.index.json {tmp_repo}/training_args.bin {tmp_repo}/trainer_state.json {tmp_repo}/all_results.json {tmp_repo}/eval_results.json {tmp_repo}/.gitattributes {tmp_repo}/train_results.json\")\n",
        "    repo = os.listdir(tmp_repo); clear_output()\n",
        "    for download in repo:    \n",
        "      link_path = f\"https://huggingface.co/{huggingface_org}/{huggingface_repo}/\"\n",
        "      if \".json\" in download or \".txt\" in download:\n",
        "        link_path += f\"raw/{huggingface_branch}/{download}\"\n",
        "      else: link_path += f\"resolve/{huggingface_branch}/{download}\"\n",
        "      os.system(f\"aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {link_path} -d /content/text-generation-webui/models/{type} -o {download}\")\n",
        "\n",
        "  def models()\n",
        "    os.chdir(main_dir)\n",
        "    if \"Pygmalion-350m(CPU)\" in model:\n",
        "      huggingface_org=\"alpindale\";huggingface_repo=\"pygm-350m-experimental\";huggingface_branch=\"main\"\n",
        "    elif \"PygmalionCoT-7b\" in model:\n",
        "      huggingface_org=\"notstoic\";huggingface_repo=\"PygmalionCoT-7b\";huggingface_branch=\"main\"\n",
        "    elif \"Wizard-Vicuna-13B-Uncensored-GPTQ\" in model:\n",
        "      huggingface_org=\"TheBloke\";huggingface_repo=\"Wizard-Vicuna-13B-Uncensored-GPTQ\";huggingface_branch= \"main\"\n",
        "    elif \"Pygmalion_6B_main_Sharded\" in model:\n",
        "      huggingface_org=\"waifu-workshop\";huggingface_repo=\"pygmalion-6b\";huggingface_branch=\"sharded\"\n",
        "    elif \"Pygmalion_6B_original_Sharded\" in model:\n",
        "      huggingface_org=\"waifu-workshop\";huggingface_repo=\"pygmalion-6b\";huggingface_branch=\"original-sharded\"\n",
        "    elif \"Pygmalion_6B_dev_Sharded\" in model:\n",
        "      huggingface_org=\"waifu-workshop\";huggingface_repo=\"pygmalion-6b\";huggingface_branch=\"dev-sharded\"\n",
        "    elif \"Pygmalion-7B\" in model:\n",
        "      huggingface_org=\"TehVenom\";huggingface_repo=\"Pygmalion-7b-Merged-Safetensors\";huggingface_branch=\"main\"\n",
        "    elif \"Metharme-7B\" in model:\n",
        "      huggingface_org=\"Imablank\";huggingface_repo=\"Metharme-7B-MERGED_WEIGHTS\";huggingface_branch=\"main\"\n",
        "    elif \"Pygmalion-13B-4bit\" in model:\n",
        "      huggingface_org=\"notstoic\";huggingface_repo=\"pygmalion-13b-4bit-128g\";huggingface_branch=\"main\"\n",
        "    elif \"Metharme-13B-4bit\" in model:\n",
        "      huggingface_org=\"TehVenom\";huggingface_repo=\"Metharme-13b-Merged\";huggingface_branch=\"main\"\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "v4qr-06xFD4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sys\n",
        "from PIL import Image\n",
        "sys.path.append(\"/\")\n",
        "import Run_WebUI\n",
        "\n",
        "from Run_WebUI import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "xo5RD4X8_CCQ",
        "outputId": "2af4c104-b868-4456-a1dc-940ebc56918f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-e6fbd8868d14>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0;36m, in \u001b[0;35m<cell line: 4>\u001b[0;36m\u001b[0m\n\u001b[0;31m    import Run_WebUI\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m\"/Run_WebUI.py\"\u001b[0;36m, line \u001b[0;32m263\u001b[0m\n\u001b[0;31m    os.system(f\"rm -rf  \"/content/text-generation-webui/models/config.yaml\"\u001b[0m\n\u001b[0m                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 263)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **‚ö†Ô∏èDelete TextGen‚ö†Ô∏è**\n",
        "!rm {main_dir}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "w2_MynLIqblq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}